{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29513307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net level\n",
    "import pNN_NetLevel_LNC_variation as pNN\n",
    "\n",
    "# packages\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "import calendar\n",
    "import numpy as np\n",
    "import config\n",
    "import training_together_variation as training \n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "torch.set_default_tensor_type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89ca9016",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = os.listdir('./dataset')\n",
    "datasets = [f for f in datasets if (f.startswith('Dataset') and f.endswith('.p'))] \n",
    "#datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36602eb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_acuteinflammation_seed_0_nonlearnable1\n",
      "| Epoch:        0 | Valid accuracy: 0.86957 | Valid loss: 0.676321149 |\n",
      "| Epoch:     5000 | Valid accuracy: 1.00000 | Valid loss: 0.000000000 |\n",
      "dataset_balancescale_seed_0_nonlearnable1\n",
      "| Epoch:        0 | Valid accuracy: 0.41935 | Valid loss: 0.713952243 |\n",
      "| Epoch:     5000 | Valid accuracy: 0.96774 | Valid loss: 0.048945166 |\n",
      "dataset_breastcancerwisc_seed_0_nonlearnable1\n",
      "| Epoch:        0 | Valid accuracy: 0.66187 | Valid loss: 0.698530138 |\n",
      "| Epoch:     5000 | Valid accuracy: 0.94964 | Valid loss: 0.090649277 |\n",
      "dataset_cardiotocography3clases_seed_0_nonlearnable1\n",
      "| Epoch:        0 | Valid accuracy: 0.74528 | Valid loss: 0.702790558 |\n",
      "| Epoch:     5000 | Valid accuracy: 0.90566 | Valid loss: 0.169152781 |\n",
      "dataset_energyy1_seed_0_nonlearnable1\n",
      "| Epoch:        0 | Valid accuracy: 0.20261 | Valid loss: 0.725977361 |\n",
      "| Epoch:     5000 | Valid accuracy: 0.84314 | Valid loss: 0.240390152 |\n",
      "| Epoch:    10000 | Valid accuracy: 0.87582 | Valid loss: 0.237709463 |\n",
      "dataset_energyy2_seed_0_nonlearnable1\n",
      "| Epoch:        0 | Valid accuracy: 0.26797 | Valid loss: 0.714964867 |\n",
      "| Epoch:     5000 | Valid accuracy: 0.91503 | Valid loss: 0.156471372 |\n",
      "dataset_iris_seed_0_nonlearnable1\n",
      "| Epoch:        0 | Valid accuracy: 0.68966 | Valid loss: 0.699851811 |\n",
      "| Epoch:     5000 | Valid accuracy: 0.93103 | Valid loss: 0.060909629 |\n",
      "dataset_mammographic_seed_0_nonlearnable1\n",
      "| Epoch:        0 | Valid accuracy: 0.56545 | Valid loss: 0.696022928 |\n",
      "| Epoch:     5000 | Valid accuracy: 0.83770 | Valid loss: 0.268224239 |\n",
      "dataset_Pendigits_seed_0_nonlearnable1\n",
      "| Epoch:        0 | Valid accuracy: 0.11146 | Valid loss: 0.808271766 |\n",
      "| Epoch:     5000 | Valid accuracy: 0.38944 | Valid loss: 0.542116046 |\n",
      "| Epoch:    10000 | Valid accuracy: 0.40127 | Valid loss: 0.538267255 |\n",
      "dataset_seeds_seed_0_nonlearnable1\n",
      "| Epoch:        0 | Valid accuracy: 0.48780 | Valid loss: 0.703708291 |\n",
      "| Epoch:     5000 | Valid accuracy: 0.92683 | Valid loss: 0.167483777 |\n",
      "dataset_tictactoe_seed_0_nonlearnable1\n",
      "| Epoch:        0 | Valid accuracy: 0.28272 | Valid loss: 0.710462570 |\n",
      "| Epoch:     5000 | Valid accuracy: 0.80105 | Valid loss: 0.342081189 |\n",
      "dataset_vertebralcolumn2clases_seed_0_nonlearnable1\n",
      "| Epoch:        0 | Valid accuracy: 0.32787 | Valid loss: 0.714037538 |\n",
      "| Epoch:     5000 | Valid accuracy: 0.86885 | Valid loss: 0.270163655 |\n",
      "| Epoch:    10000 | Valid accuracy: 0.86885 | Valid loss: 0.270237476 |\n",
      "dataset_vertebralcolumn3clases_seed_0_nonlearnable1\n",
      "| Epoch:        0 | Valid accuracy: 0.55738 | Valid loss: 0.700025201 |\n",
      "| Epoch:     5000 | Valid accuracy: 0.80328 | Valid loss: 0.303330094 |\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#result = torch.zeros((13, 1, 10))\n",
    "lrs = [0]\n",
    "for i in range(len(datasets)):\n",
    "    dataset = datasets[i]\n",
    "    datapath = os.path.join(f'./dataset/{dataset}')\n",
    "    with open(datapath, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    X_train    = data['X_train']\n",
    "    y_train    = data['y_train']\n",
    "    X_valid    = data['X_valid']\n",
    "    y_valid    = data['y_valid']\n",
    "    X_test     = data['X_test']\n",
    "    y_test     = data['y_test']\n",
    "    data_name  = data['name']\n",
    "\n",
    "    N_class    = data['n_class']\n",
    "    N_feature  = data['n_feature']\n",
    "    N_train    = X_train.shape[0]\n",
    "    N_valid    = X_valid.shape[0]\n",
    "    N_test     = X_test.shape[0]\n",
    "    \n",
    "    # generate tensordataset\n",
    "    trainset = TensorDataset(X_train, y_train)\n",
    "    validset = TensorDataset(X_valid, y_valid)\n",
    "    testset  = TensorDataset(X_test, y_test)\n",
    "\n",
    "    # batch\n",
    "    train_loader = DataLoader(trainset, batch_size=N_train)\n",
    "    valid_loader = DataLoader(validset, batch_size=N_valid)\n",
    "    test_loader  = DataLoader(testset,  batch_size=N_test)\n",
    "    \n",
    "    for j in range(len(lrs)):\n",
    "        lr = lrs[j]\n",
    "        \n",
    "        #for n in range(10):\n",
    "        seed = 0      \n",
    "            \n",
    "        setup = f'dataset_{data_name}_seed_{seed}_nonlearnable1'\n",
    "        print(setup)\n",
    "            \n",
    "        if os.path.exists(f'./results_variation/pNN_{setup}'):\n",
    "            print('File exists, pass.')\n",
    "        else:\n",
    "            torch.manual_seed(seed)\n",
    "            model = pNN.pNN([N_feature, 3, N_class], 1, 0)\n",
    "    \n",
    "            optimizer = torch.optim.Adam([{'params':model.GetParam('theta_'),'lr':1e-1},\n",
    "                                  {'params':model.GetParam('eta_'), 'lr':lr},\n",
    "                                  {'params':model.GetParam('inv_'),'lr':lr}])\n",
    "    \n",
    "    \n",
    "            NN, acc_valid = training.training_pNN(model, train_loader, valid_loader, optimizer, pNN.lossfunction)\n",
    "                #result[i,j,n] = acc_valid\n",
    "            torch.save(NN, f'./results_variation/pNN_{setup}')\n",
    "#a = {'acc_valid': result}\n",
    "#with open('acc_valid_0.0005_learnable.txt', 'wb') as file:\n",
    "    #pickle.dump(a, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ef7b4f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([13, 1, 10]),\n",
       " tensor([[[1.0000, 1.0000, 0.8696, 1.0000, 1.0000, 0.8696, 1.0000, 1.0000,\n",
       "           1.0000, 1.0000]],\n",
       " \n",
       "         [[0.8952, 0.8952, 0.9919, 0.9758, 0.9032, 0.9032, 0.9758, 0.9032,\n",
       "           0.9516, 0.9032]],\n",
       " \n",
       "         [[0.9353, 0.9209, 0.9353, 0.9353, 0.9784, 0.9712, 0.9856, 0.9712,\n",
       "           0.9281, 0.9784]],\n",
       " \n",
       "         [[0.8420, 0.8585, 0.8774, 0.8160, 0.8090, 0.8113, 0.7453, 0.8066,\n",
       "           0.8561, 0.7453]],\n",
       " \n",
       "         [[0.7974, 0.7974, 0.8431, 0.8824, 0.8105, 0.8693, 0.7974, 0.8497,\n",
       "           0.8889, 0.7974]],\n",
       " \n",
       "         [[0.9216, 0.9281, 0.7386, 0.7386, 0.9346, 0.9216, 0.9346, 0.8693,\n",
       "           0.8693, 0.9216]],\n",
       " \n",
       "         [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "           1.0000, 1.0000]],\n",
       " \n",
       "         [[0.8220, 0.8377, 0.8377, 0.8534, 0.8220, 0.8168, 0.8377, 0.8272,\n",
       "           0.8325, 0.8534]],\n",
       " \n",
       "         [[0.1115, 0.3612, 0.6324, 0.4873, 0.3790, 0.6629, 0.6815, 0.5150,\n",
       "           0.6001, 0.5510]],\n",
       " \n",
       "         [[0.9512, 0.9512, 0.9512, 0.9512, 0.9512, 0.9512, 0.9512, 0.9512,\n",
       "           0.7561, 0.9512]],\n",
       " \n",
       "         [[0.8010, 0.8482, 0.9791, 0.9319, 0.8639, 0.8639, 0.8429, 0.8743,\n",
       "           0.7801, 0.8743]],\n",
       " \n",
       "         [[0.8525, 0.8033, 0.9508, 0.9016, 0.8525, 0.9016, 0.8689, 0.8852,\n",
       "           0.7705, 0.9508]],\n",
       " \n",
       "         [[0.8525, 0.8689, 0.8852, 0.8361, 0.8033, 0.8852, 0.8852, 0.8525,\n",
       "           0.8852, 0.8525]]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d45e56e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8164)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8db693",
   "metadata": {},
   "source": [
    "## valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e35394f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "acc = torch.zeros((13, 1, 10))\n",
    "#lrs = [0.0005]\n",
    "for i in range(len(datasets)):\n",
    "    dataset = datasets[i]\n",
    "    datapath = os.path.join(f'./dataset/{dataset}')\n",
    "    with open(datapath, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    X_train    = data['X_train']\n",
    "    y_train    = data['y_train']\n",
    "    X_valid    = data['X_valid']\n",
    "    y_valid    = data['y_valid']\n",
    "    X_test     = data['X_test']\n",
    "    y_test     = data['y_test']\n",
    "    data_name  = data['name']\n",
    "\n",
    "    N_class    = data['n_class']\n",
    "    N_feature  = data['n_feature']\n",
    "    N_train    = X_train.shape[0]\n",
    "    N_valid    = X_valid.shape[0]\n",
    "    N_test     = X_test.shape[0]\n",
    "    \n",
    "    # generate tensordataset\n",
    "    trainset = TensorDataset(X_train, y_train)\n",
    "    validset = TensorDataset(X_valid, y_valid)\n",
    "    testset  = TensorDataset(X_test, y_test)\n",
    "\n",
    "    # batch\n",
    "    train_loader = DataLoader(trainset, batch_size=N_train)\n",
    "    valid_loader = DataLoader(validset, batch_size=N_valid)\n",
    "    test_loader  = DataLoader(testset,  batch_size=N_test)\n",
    "    \n",
    "    for j in range(len(lrs)):\n",
    "        lr = lrs[j]\n",
    "        \n",
    "        for n in range(10):\n",
    "            seed = n      \n",
    "            setup = f'dataset_{data_name}_seed_{seed}_nonlearnable1'\n",
    "            model = torch.load(f'./results_variation/pNN_{setup}')\n",
    "            \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for X_valid, y_valid in valid_loader:\n",
    "                \n",
    "                    prediction_valid = model(X_valid)\n",
    "                    #acc_valid = lossfunction(prediction_valid, y_valid)\n",
    "                    yhat_valid = torch.argmax(prediction_valid.data, 2)\n",
    "                \n",
    "                    yy_valid = y_valid.repeat(prediction_valid.shape[0], 1)\n",
    "                    valid_correct = torch.sum(yhat_valid == yy_valid.data)\n",
    "                    acc_valid = valid_correct / (y_valid.numel() * prediction_valid.shape[0])\n",
    "                    acc[i,j,n] = acc_valid\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcdf4bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b115084a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "           1.0000, 1.0000]],\n",
       " \n",
       "         [[0.9677, 0.8952, 0.9677, 0.8710, 0.9677, 0.9677, 0.9677, 0.8952,\n",
       "           0.9677, 0.8952]],\n",
       " \n",
       "         [[0.9496, 0.9640, 0.9712, 0.9496, 0.9496, 0.9640, 0.9712, 0.9640,\n",
       "           0.9568, 0.9496]],\n",
       " \n",
       "         [[0.8986, 0.8703, 0.9033, 0.8774, 0.9104, 0.8703, 0.8797, 0.8750,\n",
       "           0.8939, 0.8726]],\n",
       " \n",
       "         [[0.8431, 0.8758, 0.8758, 0.8889, 0.8824, 0.8824, 0.8824, 0.8824,\n",
       "           0.8431, 0.8431]],\n",
       " \n",
       "         [[0.9281, 0.9150, 0.9281, 0.9281, 0.9281, 0.9085, 0.9412, 0.9281,\n",
       "           0.9216, 0.9216]],\n",
       " \n",
       "         [[0.9655, 1.0000, 1.0000, 1.0000, 1.0000, 0.9655, 1.0000, 1.0000,\n",
       "           0.9655, 1.0000]],\n",
       " \n",
       "         [[0.8377, 0.8272, 0.8429, 0.8220, 0.8220, 0.8429, 0.8220, 0.8429,\n",
       "           0.8377, 0.8377]],\n",
       " \n",
       "         [[0.3999, 0.4249, 0.4945, 0.4877, 0.5428, 0.4759, 0.5537, 0.4595,\n",
       "           0.3999, 0.4786]],\n",
       " \n",
       "         [[0.9268, 0.9512, 0.9512, 0.9268, 0.9268, 0.9268, 0.9268, 0.9024,\n",
       "           0.9024, 0.9268]],\n",
       " \n",
       "         [[0.8325, 0.7801, 0.8010, 0.7853, 0.7906, 0.8168, 0.7853, 0.7801,\n",
       "           0.8115, 0.7801]],\n",
       " \n",
       "         [[0.8689, 0.8525, 0.8525, 0.8525, 0.8525, 0.8525, 0.8525, 0.8361,\n",
       "           0.8197, 0.8525]],\n",
       " \n",
       "         [[0.8197, 0.8197, 0.8197, 0.8689, 0.8033, 0.8033, 0.8033, 0.8689,\n",
       "           0.8689, 0.8197]]]),\n",
       " torch.Size([13, 1, 10]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc,acc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7209566b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8671)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96c2510a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 2, 4, 3, 6, 1, 2, 6, 1, 0, 0, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc[:,0,:].argmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568e06a1",
   "metadata": {},
   "source": [
    "## variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cd35c2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 10])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1 = result[:13, 0, :]\n",
    "result1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa2e6934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 6, 2, 8, 4, 0, 3, 6, 0, 2, 2, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index1 = torch.argmax(result1, dim = 1)\n",
    "index1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "380f19e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.7937), tensor(0.7937))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test= torch.zeros((13,3,2))\n",
    "test_epsilons = [0, 0.05, 0.1]\n",
    "for d, dataset in enumerate(datasets):\n",
    "        datapath = os.path.join(f'./dataset/{dataset}')\n",
    "        with open(datapath, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        X_train    = data['X_train']\n",
    "        y_train    = data['y_train']\n",
    "        X_valid    = data['X_valid']\n",
    "        y_valid    = data['y_valid']\n",
    "        X_test     = data['X_test']\n",
    "        y_test     = data['y_test']\n",
    "        data_name  = data['name']\n",
    "\n",
    "        N_class    = data['n_class']\n",
    "        N_feature  = data['n_feature']\n",
    "        N_train    = X_train.shape[0]\n",
    "        N_valid    = X_valid.shape[0]\n",
    "        N_test     = X_test.shape[0]\n",
    "\n",
    "        # generate tensordataset\n",
    "        trainset = TensorDataset(X_train, y_train)\n",
    "        validset = TensorDataset(X_valid, y_valid)\n",
    "        testset  = TensorDataset(X_test, y_test)\n",
    "\n",
    "        # batch\n",
    "        train_loader = DataLoader(trainset, batch_size=N_train)\n",
    "        valid_loader = DataLoader(validset, batch_size=N_valid)\n",
    "        test_loader  = DataLoader(testset,  batch_size=N_test)\n",
    "\n",
    "        #for s, seed in enumerate(index1):\n",
    "        setup = f'dataset_{data_name}_lr_0.001_seed_{index1[d]}_learnable'\n",
    "        #setup = f'dataset_{data_name}_lr_0.001_seed_{seed}_learnable'\n",
    "        model = torch.load(f'./results_variation/pNN_{setup}')\n",
    "        \n",
    "            \n",
    "            \n",
    "        with torch.no_grad():\n",
    "            for X_test, y_test in test_loader:\n",
    "                \n",
    "                \n",
    "                for i in range(len(test_epsilons)):\n",
    "                    test_epsilon = test_epsilons[i]\n",
    "                    if test_epsilon == 0:\n",
    "                        model.SetParameter('N', 1)\n",
    "                    else:\n",
    "                        model.SetParameter('N', 100)\n",
    "                    model.SetParameter('epsilon', test_epsilon)\n",
    "                    BASIC_variation(model, X_test, y_test)\n",
    "                    acc, std = BASIC_variation(model, X_test, y_test)\n",
    "                    results_test[d,i,0], results_test[d,i,1] = acc, std\n",
    "#results_test[d,0,0], results_test[d,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8dbf49ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000],\n",
       "        [0.9444, 0.0000],\n",
       "        [0.9643, 0.0000],\n",
       "        [0.8521, 0.0000],\n",
       "        [0.9416, 0.0000],\n",
       "        [0.9286, 0.0000],\n",
       "        [0.9677, 0.0000],\n",
       "        [0.8083, 0.0000],\n",
       "        [0.6776, 0.0000],\n",
       "        [0.8605, 0.0000],\n",
       "        [0.9688, 0.0000],\n",
       "        [0.7937, 0.0000],\n",
       "        [0.7937, 0.0000]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=results_test[:,0,:]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57db4ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8847)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:,0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24bc9b5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 0.8809,\n",
       " 0.9571,\n",
       " 0.8756,\n",
       " 0.9286,\n",
       " 0.9156,\n",
       " 0.9677,\n",
       " 0.7979,\n",
       " 0.7876,\n",
       " 0.7674,\n",
       " 1.0,\n",
       " 0.8095,\n",
       " 0.7936]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b =[1.0,0.8809,0.9571,0.8756,0.9286,0.9156,0.9677,0.7979,0.7876,0.7674,1.0,0.8095,0.7936]\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a976316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8831923076923077"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = 0\n",
    "for i in b:\n",
    "    res += i\n",
    "c = res/len(b)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "227ebb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic(prediction, y, *args, **kwargs):\n",
    "    act, idx = torch.max(prediction, dim=1)\n",
    "    corrects = (y.view(-1) == idx)\n",
    "    return corrects.float().sum().item() / y.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "730fedf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BASIC_variation(nn, x, y, *args, **kwargs):\n",
    "    prediction = nn(x)\n",
    "    N = prediction.shape[0]\n",
    "    accs = []\n",
    "    for n in range(N):\n",
    "        accs.append(basic(prediction[n,:,:], y))\n",
    "    return np.mean(accs), np.std(accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3775964",
   "metadata": {},
   "source": [
    "## 5% variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb975d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9528, 0.0758],\n",
       "        [0.8999, 0.0361],\n",
       "        [0.9361, 0.0831],\n",
       "        [0.7293, 0.1620],\n",
       "        [0.8890, 0.0800],\n",
       "        [0.8464, 0.0447],\n",
       "        [0.8252, 0.1559],\n",
       "        [0.6822, 0.1139],\n",
       "        [0.5308, 0.1287],\n",
       "        [0.8435, 0.1195],\n",
       "        [0.6460, 0.1831],\n",
       "        [0.7338, 0.0781],\n",
       "        [0.6211, 0.1686]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test[:,1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3452114f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7836, 0.2407],\n",
       "        [0.8271, 0.1272],\n",
       "        [0.7494, 0.2661],\n",
       "        [0.5813, 0.2702],\n",
       "        [0.8242, 0.1485],\n",
       "        [0.8262, 0.0579],\n",
       "        [0.7271, 0.2221],\n",
       "        [0.6078, 0.1166],\n",
       "        [0.3623, 0.1839],\n",
       "        [0.6984, 0.2225],\n",
       "        [0.5281, 0.1506],\n",
       "        [0.7008, 0.0842],\n",
       "        [0.5119, 0.1965]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test[:,2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d96548",
   "metadata": {},
   "source": [
    "##  learnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "62de1f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000],\n",
       "        [0.9565, 0.9694, 0.9621, 0.9605, 0.9605, 0.9629, 0.9613, 0.9548, 0.8685,\n",
       "         0.9540],\n",
       "        [0.9633, 0.9295, 0.9309, 0.9273, 0.9331, 0.9266, 0.9288, 0.9223, 0.9259,\n",
       "         0.9317],\n",
       "        [0.8642, 0.8625, 0.8644, 0.8663, 0.8644, 0.8566, 0.8792, 0.8710, 0.8821,\n",
       "         0.8625],\n",
       "        [0.8673, 0.8379, 0.8477, 0.8647, 0.8647, 0.8725, 0.8386, 0.8412, 0.8431,\n",
       "         0.8386],\n",
       "        [0.7373, 0.8935, 0.9046, 0.8922, 0.7386, 0.7386, 0.7386, 0.9007, 0.9137,\n",
       "         0.9085],\n",
       "        [0.9862, 0.9862, 0.9793, 0.9966, 0.9862, 0.9828, 0.9897, 0.9828, 0.9931,\n",
       "         0.9828],\n",
       "        [0.8089, 0.8110, 0.8178, 0.8136, 0.8168, 0.8105, 0.8147, 0.8047, 0.8037,\n",
       "         0.8136],\n",
       "        [0.6122, 0.6015, 0.6426, 0.4105, 0.6111, 0.5569, 0.6090, 0.4217, 0.4758,\n",
       "         0.5981],\n",
       "        [0.9317, 0.9317, 0.9488, 0.9439, 0.9195, 0.9317, 0.9341, 0.9341, 0.9415,\n",
       "         0.9439],\n",
       "        [0.8743, 0.8812, 0.8743, 0.8733, 0.8743, 0.8743, 0.8791, 0.8743, 0.8743,\n",
       "         0.8743],\n",
       "        [0.7770, 0.8115, 0.8098, 0.8262, 0.8574, 0.8082, 0.8443, 0.7918, 0.8410,\n",
       "         0.8295],\n",
       "        [0.8279, 0.7951, 0.8164, 0.8082, 0.8000, 0.8279, 0.8295, 0.8164, 0.8082,\n",
       "         0.7934]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result5 = result[:13,0,:]\n",
    "result5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4f936f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 8, 5, 8, 3, 2, 2, 2, 1, 4, 6])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index5 = torch.argmax(result5, dim = 1)\n",
    "index5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cf840473",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9832 0.05605140497793074 100\n",
      "0.8732539682539684 0.01876291862004291 100\n",
      "0.8678571428571428 0.17130799330862692 100\n",
      "0.8308685446009388 0.02903097302359705 100\n",
      "0.8784415584415587 0.024931682671042895 100\n",
      "0.8574025974025973 0.03939070440073436 100\n",
      "0.8748387096774194 0.09914095117453675 100\n",
      "0.7802072538860103 0.01975962288223707 100\n",
      "0.4758890404729421 0.07638018363583102 100\n",
      "0.8369767441860464 0.08772683646791596 100\n",
      "0.7967187499999998 0.07567496842651583 100\n",
      "0.6749206349206349 0.04889713391616547 100\n",
      "0.7785714285714284 0.0859014710945371 100\n",
      "0.8083958748669761\n"
     ]
    }
   ],
   "source": [
    "#result_test1= torch.zeros((13,100))\n",
    "acc_test5 =[]\n",
    "for d, dataset in enumerate(datasets):\n",
    "        datapath = os.path.join(f'./dataset/{dataset}')\n",
    "        with open(datapath, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        X_train    = data['X_train']\n",
    "        y_train    = data['y_train']\n",
    "        X_valid    = data['X_valid']\n",
    "        y_valid    = data['y_valid']\n",
    "        X_test     = data['X_test']\n",
    "        y_test     = data['y_test']\n",
    "        data_name  = data['name']\n",
    "\n",
    "        N_class    = data['n_class']\n",
    "        N_feature  = data['n_feature']\n",
    "        N_train    = X_train.shape[0]\n",
    "        N_valid    = X_valid.shape[0]\n",
    "        N_test     = X_test.shape[0]\n",
    "\n",
    "        # generate tensordataset\n",
    "        trainset = TensorDataset(X_train, y_train)\n",
    "        validset = TensorDataset(X_valid, y_valid)\n",
    "        testset  = TensorDataset(X_test, y_test)\n",
    "\n",
    "        # batch\n",
    "        train_loader = DataLoader(trainset, batch_size=N_train)\n",
    "        valid_loader = DataLoader(validset, batch_size=N_valid)\n",
    "        test_loader  = DataLoader(testset,  batch_size=N_test)\n",
    "\n",
    "        #for s, seed in enumerate(index1):\n",
    "        setup =  f'dataset\\uf022{data_name}_epsilon\\uf0220.1_seed\\uf022{index5[d]}'\n",
    "        model = torch.load(f'./results_variation/pNN_{setup}')\n",
    "            \n",
    "            \n",
    "        with torch.no_grad():\n",
    "            for X_test, y_test in test_loader:\n",
    "                \n",
    "            #model.SetParameter('N', config.N_test)\n",
    "            #model.SetParameter('epsilon', test_epsilon)\n",
    "                accs = []\n",
    "                for i in range(10):\n",
    "                    BASIC_variation(model, X_test, y_test, accs)\n",
    "                    #accs = BASIC_variation(model, X_test, y_test, accs)\n",
    "                #acc, std, length = BASIC_variation(model, X_test, y_test, accs)\n",
    "                acc_test5.append(np.mean(accs))\n",
    "                print(np.mean(accs), np.std(accs), len(accs))\n",
    "            #results[d,s,e,0], results[d,s,e,1] = acc, std\n",
    "print(np.mean(acc_test5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b300ea77",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'pLayer' object has no attribute 'SetParameter'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X_test, y_test \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[1;32m---> 40\u001b[0m         \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSetParameter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mN\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(test_epsilons)):\n\u001b[0;32m     42\u001b[0m             test_epsilon \u001b[38;5;241m=\u001b[39m test_epsilons\n",
      "File \u001b[1;32m~\\Desktop\\Learnable_Nonlinear_Circuits\\Learnable_Nonlinear_Circuits_model\\pNN_NetLevel_LNC_variation.py:216\u001b[0m, in \u001b[0;36mpNN.SetParameter\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mN \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel:\n\u001b[1;32m--> 216\u001b[0m         \u001b[43ml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSetParameter\u001b[49m(name, value)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepsilon\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel:\n",
      "File \u001b[1;32m~\\.conda\\envs\\PrintedNeuromophics\\lib\\site-packages\\torch\\nn\\modules\\module.py:1207\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1206\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1207\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1208\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'pLayer' object has no attribute 'SetParameter'"
     ]
    }
   ],
   "source": [
    "results_test= torch.zeros((13,2,2))\n",
    "test_epsilons = [0.05, 0.1]\n",
    "for d, dataset in enumerate(datasets):\n",
    "        datapath = os.path.join(f'./dataset/{dataset}')\n",
    "        with open(datapath, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        X_train    = data['X_train']\n",
    "        y_train    = data['y_train']\n",
    "        X_valid    = data['X_valid']\n",
    "        y_valid    = data['y_valid']\n",
    "        X_test     = data['X_test']\n",
    "        y_test     = data['y_test']\n",
    "        data_name  = data['name']\n",
    "\n",
    "        N_class    = data['n_class']\n",
    "        N_feature  = data['n_feature']\n",
    "        N_train    = X_train.shape[0]\n",
    "        N_valid    = X_valid.shape[0]\n",
    "        N_test     = X_test.shape[0]\n",
    "\n",
    "        # generate tensordataset\n",
    "        trainset = TensorDataset(X_train, y_train)\n",
    "        validset = TensorDataset(X_valid, y_valid)\n",
    "        testset  = TensorDataset(X_test, y_test)\n",
    "\n",
    "        # batch\n",
    "        train_loader = DataLoader(trainset, batch_size=N_train)\n",
    "        valid_loader = DataLoader(validset, batch_size=N_valid)\n",
    "        test_loader  = DataLoader(testset,  batch_size=N_test)\n",
    "\n",
    "        #for s, seed in enumerate(index1):\n",
    "        setup = f'dataset_{data_name}__lr_0.0005_seed_{index1[d]}_learnable'\n",
    "        model = torch.load(f'./results_variation/pNN_{setup}')\n",
    "        \n",
    "            \n",
    "            \n",
    "        with torch.no_grad():\n",
    "            for X_test, y_test in test_loader:\n",
    "                \n",
    "                model.SetParameter('N', 100)\n",
    "                for i in range(len(test_epsilons)):\n",
    "                    test_epsilon = test_epsilons\n",
    "                    model.SetParameter('epsilon', test_epsilon)\n",
    "                    BASIC_variation(model, X_test, y_test, accs)\n",
    "                    acc, std = BASIC_variation(model, X_test, y_test)\n",
    "                    results_test[d,i,0], results_test[d,i,1] = acc, std\n",
    "results_test[d,0,0], results_test[d,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66217faa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c51ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('invWithoutR1234_dataset.txt', 'rb') as file2:\n",
    "    b = pickle.load(file2)\n",
    "YY = b['Y']\n",
    "YY.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
