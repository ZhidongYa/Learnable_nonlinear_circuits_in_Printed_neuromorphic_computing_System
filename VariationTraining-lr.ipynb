{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29513307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net level\n",
    "import pNN_NetLevel_LNC_variation as pNN\n",
    "\n",
    "# packages\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "import calendar\n",
    "import numpy as np\n",
    "import config\n",
    "import training_together_variation as training \n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "torch.set_default_tensor_type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89ca9016",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = os.listdir('./dataset')\n",
    "datasets = [f for f in datasets if (f.startswith('Dataset') and f.endswith('.p'))] \n",
    "#datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36602eb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_acuteinflammation_epsilon_0.1_lr_0.05_seed_0\n",
      "File exists, pass.\n",
      "dataset_acuteinflammation_epsilon_0.1_lr_0.05_seed_1\n",
      "File exists, pass.\n",
      "dataset_acuteinflammation_epsilon_0.1_lr_0.05_seed_2\n",
      "File exists, pass.\n",
      "dataset_acuteinflammation_epsilon_0.1_lr_0.05_seed_3\n",
      "File exists, pass.\n",
      "dataset_acuteinflammation_epsilon_0.1_lr_0.05_seed_4\n",
      "File exists, pass.\n",
      "dataset_acuteinflammation_epsilon_0.1_lr_0.05_seed_5\n",
      "File exists, pass.\n",
      "dataset_acuteinflammation_epsilon_0.1_lr_0.05_seed_6\n",
      "File exists, pass.\n",
      "dataset_acuteinflammation_epsilon_0.1_lr_0.05_seed_7\n",
      "File exists, pass.\n",
      "dataset_acuteinflammation_epsilon_0.1_lr_0.05_seed_8\n",
      "File exists, pass.\n",
      "dataset_acuteinflammation_epsilon_0.1_lr_0.05_seed_9\n",
      "File exists, pass.\n",
      "dataset_acuteinflammation_epsilon_0.1_lr_0.01_seed_0\n",
      "File exists, pass.\n",
      "dataset_acuteinflammation_epsilon_0.1_lr_0.01_seed_1\n",
      "File exists, pass.\n",
      "dataset_acuteinflammation_epsilon_0.1_lr_0.01_seed_2\n",
      "File exists, pass.\n",
      "dataset_acuteinflammation_epsilon_0.1_lr_0.01_seed_3\n",
      "File exists, pass.\n",
      "dataset_acuteinflammation_epsilon_0.1_lr_0.01_seed_4\n",
      "File exists, pass.\n",
      "dataset_acuteinflammation_epsilon_0.1_lr_0.01_seed_5\n",
      "File exists, pass.\n",
      "dataset_acuteinflammation_epsilon_0.1_lr_0.01_seed_6\n",
      "File exists, pass.\n",
      "dataset_acuteinflammation_epsilon_0.1_lr_0.01_seed_7\n",
      "File exists, pass.\n",
      "dataset_acuteinflammation_epsilon_0.1_lr_0.01_seed_8\n",
      "File exists, pass.\n",
      "dataset_acuteinflammation_epsilon_0.1_lr_0.01_seed_9\n",
      "File exists, pass.\n",
      "dataset_acuteinflammation_epsilon_0.1_lr_0.005_seed_0\n",
      "File exists, pass.\n",
      "dataset_acuteinflammation_epsilon_0.1_lr_0.005_seed_1\n",
      "File exists, pass.\n",
      "dataset_acuteinflammation_epsilon_0.1_lr_0.005_seed_2\n",
      "File exists, pass.\n",
      "dataset_acuteinflammation_epsilon_0.1_lr_0.005_seed_3\n",
      "File exists, pass.\n",
      "dataset_acuteinflammation_epsilon_0.1_lr_0.005_seed_4\n",
      "File exists, pass.\n",
      "dataset_acuteinflammation_epsilon_0.1_lr_0.005_seed_5\n",
      "File exists, pass.\n",
      "dataset_acuteinflammation_epsilon_0.1_lr_0.005_seed_6\n",
      "File exists, pass.\n",
      "dataset_acuteinflammation_epsilon_0.1_lr_0.005_seed_7\n",
      "File exists, pass.\n",
      "dataset_acuteinflammation_epsilon_0.1_lr_0.005_seed_8\n",
      "File exists, pass.\n",
      "dataset_acuteinflammation_epsilon_0.1_lr_0.005_seed_9\n",
      "File exists, pass.\n",
      "dataset_acuteinflammation_epsilon_0.1_lr_0.001_seed_0\n",
      "File exists, pass.\n",
      "dataset_acuteinflammation_epsilon_0.1_lr_0.001_seed_1\n",
      "File exists, pass.\n",
      "dataset_acuteinflammation_epsilon_0.1_lr_0.001_seed_2\n",
      "File exists, pass.\n",
      "dataset_acuteinflammation_epsilon_0.1_lr_0.001_seed_3\n",
      "File exists, pass.\n",
      "dataset_acuteinflammation_epsilon_0.1_lr_0.001_seed_4\n",
      "File exists, pass.\n",
      "dataset_acuteinflammation_epsilon_0.1_lr_0.001_seed_5\n",
      "File exists, pass.\n",
      "dataset_acuteinflammation_epsilon_0.1_lr_0.001_seed_6\n",
      "File exists, pass.\n",
      "dataset_acuteinflammation_epsilon_0.1_lr_0.001_seed_7\n",
      "File exists, pass.\n",
      "dataset_acuteinflammation_epsilon_0.1_lr_0.001_seed_8\n",
      "File exists, pass.\n",
      "dataset_acuteinflammation_epsilon_0.1_lr_0.001_seed_9\n",
      "File exists, pass.\n",
      "dataset_balancescale_epsilon_0.1_lr_0.05_seed_0\n",
      "File exists, pass.\n",
      "dataset_balancescale_epsilon_0.1_lr_0.05_seed_1\n",
      "File exists, pass.\n",
      "dataset_balancescale_epsilon_0.1_lr_0.05_seed_2\n",
      "File exists, pass.\n",
      "dataset_balancescale_epsilon_0.1_lr_0.05_seed_3\n",
      "File exists, pass.\n",
      "dataset_balancescale_epsilon_0.1_lr_0.05_seed_4\n",
      "File exists, pass.\n",
      "dataset_balancescale_epsilon_0.1_lr_0.05_seed_5\n",
      "File exists, pass.\n",
      "dataset_balancescale_epsilon_0.1_lr_0.05_seed_6\n",
      "File exists, pass.\n",
      "dataset_balancescale_epsilon_0.1_lr_0.05_seed_7\n",
      "File exists, pass.\n",
      "dataset_balancescale_epsilon_0.1_lr_0.05_seed_8\n",
      "File exists, pass.\n",
      "dataset_balancescale_epsilon_0.1_lr_0.05_seed_9\n",
      "File exists, pass.\n",
      "dataset_balancescale_epsilon_0.1_lr_0.01_seed_0\n",
      "File exists, pass.\n",
      "dataset_balancescale_epsilon_0.1_lr_0.01_seed_1\n",
      "File exists, pass.\n",
      "dataset_balancescale_epsilon_0.1_lr_0.01_seed_2\n",
      "File exists, pass.\n",
      "dataset_balancescale_epsilon_0.1_lr_0.01_seed_3\n",
      "File exists, pass.\n",
      "dataset_balancescale_epsilon_0.1_lr_0.01_seed_4\n",
      "File exists, pass.\n",
      "dataset_balancescale_epsilon_0.1_lr_0.01_seed_5\n",
      "File exists, pass.\n",
      "dataset_balancescale_epsilon_0.1_lr_0.01_seed_6\n",
      "File exists, pass.\n",
      "dataset_balancescale_epsilon_0.1_lr_0.01_seed_7\n",
      "File exists, pass.\n",
      "dataset_balancescale_epsilon_0.1_lr_0.01_seed_8\n",
      "File exists, pass.\n",
      "dataset_balancescale_epsilon_0.1_lr_0.01_seed_9\n",
      "File exists, pass.\n",
      "dataset_balancescale_epsilon_0.1_lr_0.005_seed_0\n",
      "File exists, pass.\n",
      "dataset_balancescale_epsilon_0.1_lr_0.005_seed_1\n",
      "File exists, pass.\n",
      "dataset_balancescale_epsilon_0.1_lr_0.005_seed_2\n",
      "File exists, pass.\n",
      "dataset_balancescale_epsilon_0.1_lr_0.005_seed_3\n",
      "File exists, pass.\n",
      "dataset_balancescale_epsilon_0.1_lr_0.005_seed_4\n",
      "File exists, pass.\n",
      "dataset_balancescale_epsilon_0.1_lr_0.005_seed_5\n",
      "File exists, pass.\n",
      "dataset_balancescale_epsilon_0.1_lr_0.005_seed_6\n",
      "File exists, pass.\n",
      "dataset_balancescale_epsilon_0.1_lr_0.005_seed_7\n",
      "File exists, pass.\n",
      "dataset_balancescale_epsilon_0.1_lr_0.005_seed_8\n",
      "File exists, pass.\n",
      "dataset_balancescale_epsilon_0.1_lr_0.005_seed_9\n",
      "File exists, pass.\n",
      "dataset_balancescale_epsilon_0.1_lr_0.001_seed_0\n",
      "File exists, pass.\n",
      "dataset_balancescale_epsilon_0.1_lr_0.001_seed_1\n",
      "File exists, pass.\n",
      "dataset_balancescale_epsilon_0.1_lr_0.001_seed_2\n",
      "File exists, pass.\n",
      "dataset_balancescale_epsilon_0.1_lr_0.001_seed_3\n",
      "File exists, pass.\n",
      "dataset_balancescale_epsilon_0.1_lr_0.001_seed_4\n",
      "File exists, pass.\n",
      "dataset_balancescale_epsilon_0.1_lr_0.001_seed_5\n",
      "File exists, pass.\n",
      "dataset_balancescale_epsilon_0.1_lr_0.001_seed_6\n",
      "File exists, pass.\n",
      "dataset_balancescale_epsilon_0.1_lr_0.001_seed_7\n",
      "File exists, pass.\n",
      "dataset_balancescale_epsilon_0.1_lr_0.001_seed_8\n",
      "File exists, pass.\n",
      "dataset_balancescale_epsilon_0.1_lr_0.001_seed_9\n",
      "File exists, pass.\n",
      "dataset_breastcancerwisc_epsilon_0.1_lr_0.05_seed_0\n",
      "File exists, pass.\n",
      "dataset_breastcancerwisc_epsilon_0.1_lr_0.05_seed_1\n",
      "File exists, pass.\n",
      "dataset_breastcancerwisc_epsilon_0.1_lr_0.05_seed_2\n",
      "File exists, pass.\n",
      "dataset_breastcancerwisc_epsilon_0.1_lr_0.05_seed_3\n",
      "File exists, pass.\n",
      "dataset_breastcancerwisc_epsilon_0.1_lr_0.05_seed_4\n",
      "File exists, pass.\n",
      "dataset_breastcancerwisc_epsilon_0.1_lr_0.05_seed_5\n",
      "File exists, pass.\n",
      "dataset_breastcancerwisc_epsilon_0.1_lr_0.05_seed_6\n",
      "File exists, pass.\n",
      "dataset_breastcancerwisc_epsilon_0.1_lr_0.05_seed_7\n",
      "File exists, pass.\n",
      "dataset_breastcancerwisc_epsilon_0.1_lr_0.05_seed_8\n",
      "File exists, pass.\n",
      "dataset_breastcancerwisc_epsilon_0.1_lr_0.05_seed_9\n",
      "File exists, pass.\n",
      "dataset_breastcancerwisc_epsilon_0.1_lr_0.01_seed_0\n",
      "File exists, pass.\n",
      "dataset_breastcancerwisc_epsilon_0.1_lr_0.01_seed_1\n",
      "File exists, pass.\n",
      "dataset_breastcancerwisc_epsilon_0.1_lr_0.01_seed_2\n",
      "File exists, pass.\n",
      "dataset_breastcancerwisc_epsilon_0.1_lr_0.01_seed_3\n",
      "File exists, pass.\n",
      "dataset_breastcancerwisc_epsilon_0.1_lr_0.01_seed_4\n",
      "File exists, pass.\n",
      "dataset_breastcancerwisc_epsilon_0.1_lr_0.01_seed_5\n",
      "File exists, pass.\n",
      "dataset_breastcancerwisc_epsilon_0.1_lr_0.01_seed_6\n",
      "File exists, pass.\n",
      "dataset_breastcancerwisc_epsilon_0.1_lr_0.01_seed_7\n",
      "File exists, pass.\n",
      "dataset_breastcancerwisc_epsilon_0.1_lr_0.01_seed_8\n",
      "File exists, pass.\n",
      "dataset_breastcancerwisc_epsilon_0.1_lr_0.01_seed_9\n",
      "File exists, pass.\n",
      "dataset_breastcancerwisc_epsilon_0.1_lr_0.005_seed_0\n",
      "File exists, pass.\n",
      "dataset_breastcancerwisc_epsilon_0.1_lr_0.005_seed_1\n",
      "File exists, pass.\n",
      "dataset_breastcancerwisc_epsilon_0.1_lr_0.005_seed_2\n",
      "File exists, pass.\n",
      "dataset_breastcancerwisc_epsilon_0.1_lr_0.005_seed_3\n",
      "File exists, pass.\n",
      "dataset_breastcancerwisc_epsilon_0.1_lr_0.005_seed_4\n",
      "File exists, pass.\n",
      "dataset_breastcancerwisc_epsilon_0.1_lr_0.005_seed_5\n",
      "File exists, pass.\n",
      "dataset_breastcancerwisc_epsilon_0.1_lr_0.005_seed_6\n",
      "File exists, pass.\n",
      "dataset_breastcancerwisc_epsilon_0.1_lr_0.005_seed_7\n",
      "File exists, pass.\n",
      "dataset_breastcancerwisc_epsilon_0.1_lr_0.005_seed_8\n",
      "File exists, pass.\n",
      "dataset_breastcancerwisc_epsilon_0.1_lr_0.005_seed_9\n",
      "File exists, pass.\n",
      "dataset_breastcancerwisc_epsilon_0.1_lr_0.001_seed_0\n",
      "File exists, pass.\n",
      "dataset_breastcancerwisc_epsilon_0.1_lr_0.001_seed_1\n",
      "File exists, pass.\n",
      "dataset_breastcancerwisc_epsilon_0.1_lr_0.001_seed_2\n",
      "File exists, pass.\n",
      "dataset_breastcancerwisc_epsilon_0.1_lr_0.001_seed_3\n",
      "File exists, pass.\n",
      "dataset_breastcancerwisc_epsilon_0.1_lr_0.001_seed_4\n",
      "File exists, pass.\n",
      "dataset_breastcancerwisc_epsilon_0.1_lr_0.001_seed_5\n",
      "File exists, pass.\n",
      "dataset_breastcancerwisc_epsilon_0.1_lr_0.001_seed_6\n",
      "File exists, pass.\n",
      "dataset_breastcancerwisc_epsilon_0.1_lr_0.001_seed_7\n",
      "File exists, pass.\n",
      "dataset_breastcancerwisc_epsilon_0.1_lr_0.001_seed_8\n",
      "File exists, pass.\n",
      "dataset_breastcancerwisc_epsilon_0.1_lr_0.001_seed_9\n",
      "File exists, pass.\n",
      "dataset_cardiotocography3clases_epsilon_0.1_lr_0.05_seed_0\n",
      "File exists, pass.\n",
      "dataset_cardiotocography3clases_epsilon_0.1_lr_0.05_seed_1\n",
      "File exists, pass.\n",
      "dataset_cardiotocography3clases_epsilon_0.1_lr_0.05_seed_2\n",
      "File exists, pass.\n",
      "dataset_cardiotocography3clases_epsilon_0.1_lr_0.05_seed_3\n",
      "File exists, pass.\n",
      "dataset_cardiotocography3clases_epsilon_0.1_lr_0.05_seed_4\n",
      "File exists, pass.\n",
      "dataset_cardiotocography3clases_epsilon_0.1_lr_0.05_seed_5\n",
      "File exists, pass.\n",
      "dataset_cardiotocography3clases_epsilon_0.1_lr_0.05_seed_6\n",
      "File exists, pass.\n",
      "dataset_cardiotocography3clases_epsilon_0.1_lr_0.05_seed_7\n",
      "File exists, pass.\n",
      "dataset_cardiotocography3clases_epsilon_0.1_lr_0.05_seed_8\n",
      "File exists, pass.\n",
      "dataset_cardiotocography3clases_epsilon_0.1_lr_0.05_seed_9\n",
      "File exists, pass.\n",
      "dataset_cardiotocography3clases_epsilon_0.1_lr_0.01_seed_0\n",
      "File exists, pass.\n",
      "dataset_cardiotocography3clases_epsilon_0.1_lr_0.01_seed_1\n",
      "File exists, pass.\n",
      "dataset_cardiotocography3clases_epsilon_0.1_lr_0.01_seed_2\n",
      "File exists, pass.\n",
      "dataset_cardiotocography3clases_epsilon_0.1_lr_0.01_seed_3\n",
      "File exists, pass.\n",
      "dataset_cardiotocography3clases_epsilon_0.1_lr_0.01_seed_4\n",
      "File exists, pass.\n",
      "dataset_cardiotocography3clases_epsilon_0.1_lr_0.01_seed_5\n",
      "File exists, pass.\n",
      "dataset_cardiotocography3clases_epsilon_0.1_lr_0.01_seed_6\n",
      "File exists, pass.\n",
      "dataset_cardiotocography3clases_epsilon_0.1_lr_0.01_seed_7\n",
      "File exists, pass.\n",
      "dataset_cardiotocography3clases_epsilon_0.1_lr_0.01_seed_8\n",
      "File exists, pass.\n",
      "dataset_cardiotocography3clases_epsilon_0.1_lr_0.01_seed_9\n",
      "File exists, pass.\n",
      "dataset_cardiotocography3clases_epsilon_0.1_lr_0.005_seed_0\n",
      "File exists, pass.\n",
      "dataset_cardiotocography3clases_epsilon_0.1_lr_0.005_seed_1\n",
      "File exists, pass.\n",
      "dataset_cardiotocography3clases_epsilon_0.1_lr_0.005_seed_2\n",
      "File exists, pass.\n",
      "dataset_cardiotocography3clases_epsilon_0.1_lr_0.005_seed_3\n",
      "File exists, pass.\n",
      "dataset_cardiotocography3clases_epsilon_0.1_lr_0.005_seed_4\n",
      "File exists, pass.\n",
      "dataset_cardiotocography3clases_epsilon_0.1_lr_0.005_seed_5\n",
      "File exists, pass.\n",
      "dataset_cardiotocography3clases_epsilon_0.1_lr_0.005_seed_6\n",
      "File exists, pass.\n",
      "dataset_cardiotocography3clases_epsilon_0.1_lr_0.005_seed_7\n",
      "File exists, pass.\n",
      "dataset_cardiotocography3clases_epsilon_0.1_lr_0.005_seed_8\n",
      "File exists, pass.\n",
      "dataset_cardiotocography3clases_epsilon_0.1_lr_0.005_seed_9\n",
      "File exists, pass.\n",
      "dataset_cardiotocography3clases_epsilon_0.1_lr_0.001_seed_0\n",
      "File exists, pass.\n",
      "dataset_cardiotocography3clases_epsilon_0.1_lr_0.001_seed_1\n",
      "File exists, pass.\n",
      "dataset_cardiotocography3clases_epsilon_0.1_lr_0.001_seed_2\n",
      "File exists, pass.\n",
      "dataset_cardiotocography3clases_epsilon_0.1_lr_0.001_seed_3\n",
      "File exists, pass.\n",
      "dataset_cardiotocography3clases_epsilon_0.1_lr_0.001_seed_4\n",
      "File exists, pass.\n",
      "dataset_cardiotocography3clases_epsilon_0.1_lr_0.001_seed_5\n",
      "File exists, pass.\n",
      "dataset_cardiotocography3clases_epsilon_0.1_lr_0.001_seed_6\n",
      "File exists, pass.\n",
      "dataset_cardiotocography3clases_epsilon_0.1_lr_0.001_seed_7\n",
      "File exists, pass.\n",
      "dataset_cardiotocography3clases_epsilon_0.1_lr_0.001_seed_8\n",
      "File exists, pass.\n",
      "dataset_cardiotocography3clases_epsilon_0.1_lr_0.001_seed_9\n",
      "File exists, pass.\n",
      "dataset_energyy1_epsilon_0.1_lr_0.05_seed_0\n",
      "File exists, pass.\n",
      "dataset_energyy1_epsilon_0.1_lr_0.05_seed_1\n",
      "File exists, pass.\n",
      "dataset_energyy1_epsilon_0.1_lr_0.05_seed_2\n",
      "File exists, pass.\n",
      "dataset_energyy1_epsilon_0.1_lr_0.05_seed_3\n",
      "File exists, pass.\n",
      "dataset_energyy1_epsilon_0.1_lr_0.05_seed_4\n",
      "File exists, pass.\n",
      "dataset_energyy1_epsilon_0.1_lr_0.05_seed_5\n",
      "File exists, pass.\n",
      "dataset_energyy1_epsilon_0.1_lr_0.05_seed_6\n",
      "File exists, pass.\n",
      "dataset_energyy1_epsilon_0.1_lr_0.05_seed_7\n",
      "File exists, pass.\n",
      "dataset_energyy1_epsilon_0.1_lr_0.05_seed_8\n",
      "File exists, pass.\n",
      "dataset_energyy1_epsilon_0.1_lr_0.05_seed_9\n",
      "File exists, pass.\n",
      "dataset_energyy1_epsilon_0.1_lr_0.01_seed_0\n",
      "File exists, pass.\n",
      "dataset_energyy1_epsilon_0.1_lr_0.01_seed_1\n",
      "File exists, pass.\n",
      "dataset_energyy1_epsilon_0.1_lr_0.01_seed_2\n",
      "File exists, pass.\n",
      "dataset_energyy1_epsilon_0.1_lr_0.01_seed_3\n",
      "File exists, pass.\n",
      "dataset_energyy1_epsilon_0.1_lr_0.01_seed_4\n",
      "File exists, pass.\n",
      "dataset_energyy1_epsilon_0.1_lr_0.01_seed_5\n",
      "File exists, pass.\n",
      "dataset_energyy1_epsilon_0.1_lr_0.01_seed_6\n",
      "File exists, pass.\n",
      "dataset_energyy1_epsilon_0.1_lr_0.01_seed_7\n",
      "File exists, pass.\n",
      "dataset_energyy1_epsilon_0.1_lr_0.01_seed_8\n",
      "File exists, pass.\n",
      "dataset_energyy1_epsilon_0.1_lr_0.01_seed_9\n",
      "File exists, pass.\n",
      "dataset_energyy1_epsilon_0.1_lr_0.005_seed_0\n",
      "File exists, pass.\n",
      "dataset_energyy1_epsilon_0.1_lr_0.005_seed_1\n",
      "File exists, pass.\n",
      "dataset_energyy1_epsilon_0.1_lr_0.005_seed_2\n",
      "File exists, pass.\n",
      "dataset_energyy1_epsilon_0.1_lr_0.005_seed_3\n",
      "File exists, pass.\n",
      "dataset_energyy1_epsilon_0.1_lr_0.005_seed_4\n",
      "File exists, pass.\n",
      "dataset_energyy1_epsilon_0.1_lr_0.005_seed_5\n",
      "File exists, pass.\n",
      "dataset_energyy1_epsilon_0.1_lr_0.005_seed_6\n",
      "File exists, pass.\n",
      "dataset_energyy1_epsilon_0.1_lr_0.005_seed_7\n",
      "File exists, pass.\n",
      "dataset_energyy1_epsilon_0.1_lr_0.005_seed_8\n",
      "File exists, pass.\n",
      "dataset_energyy1_epsilon_0.1_lr_0.005_seed_9\n",
      "File exists, pass.\n",
      "dataset_energyy1_epsilon_0.1_lr_0.001_seed_0\n",
      "File exists, pass.\n",
      "dataset_energyy1_epsilon_0.1_lr_0.001_seed_1\n",
      "File exists, pass.\n",
      "dataset_energyy1_epsilon_0.1_lr_0.001_seed_2\n",
      "File exists, pass.\n",
      "dataset_energyy1_epsilon_0.1_lr_0.001_seed_3\n",
      "File exists, pass.\n",
      "dataset_energyy1_epsilon_0.1_lr_0.001_seed_4\n",
      "File exists, pass.\n",
      "dataset_energyy1_epsilon_0.1_lr_0.001_seed_5\n",
      "File exists, pass.\n",
      "dataset_energyy1_epsilon_0.1_lr_0.001_seed_6\n",
      "File exists, pass.\n",
      "dataset_energyy1_epsilon_0.1_lr_0.001_seed_7\n",
      "File exists, pass.\n",
      "dataset_energyy1_epsilon_0.1_lr_0.001_seed_8\n",
      "File exists, pass.\n",
      "dataset_energyy1_epsilon_0.1_lr_0.001_seed_9\n",
      "File exists, pass.\n",
      "dataset_energyy2_epsilon_0.1_lr_0.05_seed_0\n",
      "File exists, pass.\n",
      "dataset_energyy2_epsilon_0.1_lr_0.05_seed_1\n",
      "File exists, pass.\n",
      "dataset_energyy2_epsilon_0.1_lr_0.05_seed_2\n",
      "File exists, pass.\n",
      "dataset_energyy2_epsilon_0.1_lr_0.05_seed_3\n",
      "File exists, pass.\n",
      "dataset_energyy2_epsilon_0.1_lr_0.05_seed_4\n",
      "File exists, pass.\n",
      "dataset_energyy2_epsilon_0.1_lr_0.05_seed_5\n",
      "File exists, pass.\n",
      "dataset_energyy2_epsilon_0.1_lr_0.05_seed_6\n",
      "File exists, pass.\n",
      "dataset_energyy2_epsilon_0.1_lr_0.05_seed_7\n",
      "File exists, pass.\n",
      "dataset_energyy2_epsilon_0.1_lr_0.05_seed_8\n",
      "File exists, pass.\n",
      "dataset_energyy2_epsilon_0.1_lr_0.05_seed_9\n",
      "File exists, pass.\n",
      "dataset_energyy2_epsilon_0.1_lr_0.01_seed_0\n",
      "File exists, pass.\n",
      "dataset_energyy2_epsilon_0.1_lr_0.01_seed_1\n",
      "File exists, pass.\n",
      "dataset_energyy2_epsilon_0.1_lr_0.01_seed_2\n",
      "File exists, pass.\n",
      "dataset_energyy2_epsilon_0.1_lr_0.01_seed_3\n",
      "File exists, pass.\n",
      "dataset_energyy2_epsilon_0.1_lr_0.01_seed_4\n",
      "File exists, pass.\n",
      "dataset_energyy2_epsilon_0.1_lr_0.01_seed_5\n",
      "File exists, pass.\n",
      "dataset_energyy2_epsilon_0.1_lr_0.01_seed_6\n",
      "File exists, pass.\n",
      "dataset_energyy2_epsilon_0.1_lr_0.01_seed_7\n",
      "File exists, pass.\n",
      "dataset_energyy2_epsilon_0.1_lr_0.01_seed_8\n",
      "File exists, pass.\n",
      "dataset_energyy2_epsilon_0.1_lr_0.01_seed_9\n",
      "File exists, pass.\n",
      "dataset_energyy2_epsilon_0.1_lr_0.005_seed_0\n",
      "File exists, pass.\n",
      "dataset_energyy2_epsilon_0.1_lr_0.005_seed_1\n",
      "File exists, pass.\n",
      "dataset_energyy2_epsilon_0.1_lr_0.005_seed_2\n",
      "File exists, pass.\n",
      "dataset_energyy2_epsilon_0.1_lr_0.005_seed_3\n",
      "File exists, pass.\n",
      "dataset_energyy2_epsilon_0.1_lr_0.005_seed_4\n",
      "File exists, pass.\n",
      "dataset_energyy2_epsilon_0.1_lr_0.005_seed_5\n",
      "File exists, pass.\n",
      "dataset_energyy2_epsilon_0.1_lr_0.005_seed_6\n",
      "File exists, pass.\n",
      "dataset_energyy2_epsilon_0.1_lr_0.005_seed_7\n",
      "File exists, pass.\n",
      "dataset_energyy2_epsilon_0.1_lr_0.005_seed_8\n",
      "File exists, pass.\n",
      "dataset_energyy2_epsilon_0.1_lr_0.005_seed_9\n",
      "File exists, pass.\n",
      "dataset_energyy2_epsilon_0.1_lr_0.001_seed_0\n",
      "File exists, pass.\n",
      "dataset_energyy2_epsilon_0.1_lr_0.001_seed_1\n",
      "File exists, pass.\n",
      "dataset_energyy2_epsilon_0.1_lr_0.001_seed_2\n",
      "File exists, pass.\n",
      "dataset_energyy2_epsilon_0.1_lr_0.001_seed_3\n",
      "File exists, pass.\n",
      "dataset_energyy2_epsilon_0.1_lr_0.001_seed_4\n",
      "File exists, pass.\n",
      "dataset_energyy2_epsilon_0.1_lr_0.001_seed_5\n",
      "File exists, pass.\n",
      "dataset_energyy2_epsilon_0.1_lr_0.001_seed_6\n",
      "File exists, pass.\n",
      "dataset_energyy2_epsilon_0.1_lr_0.001_seed_7\n",
      "File exists, pass.\n",
      "dataset_energyy2_epsilon_0.1_lr_0.001_seed_8\n",
      "File exists, pass.\n",
      "dataset_energyy2_epsilon_0.1_lr_0.001_seed_9\n",
      "File exists, pass.\n",
      "dataset_iris_epsilon_0.1_lr_0.05_seed_0\n",
      "File exists, pass.\n",
      "dataset_iris_epsilon_0.1_lr_0.05_seed_1\n",
      "File exists, pass.\n",
      "dataset_iris_epsilon_0.1_lr_0.05_seed_2\n",
      "File exists, pass.\n",
      "dataset_iris_epsilon_0.1_lr_0.05_seed_3\n",
      "File exists, pass.\n",
      "dataset_iris_epsilon_0.1_lr_0.05_seed_4\n",
      "File exists, pass.\n",
      "dataset_iris_epsilon_0.1_lr_0.05_seed_5\n",
      "File exists, pass.\n",
      "dataset_iris_epsilon_0.1_lr_0.05_seed_6\n",
      "File exists, pass.\n",
      "dataset_iris_epsilon_0.1_lr_0.05_seed_7\n",
      "File exists, pass.\n",
      "dataset_iris_epsilon_0.1_lr_0.05_seed_8\n",
      "File exists, pass.\n",
      "dataset_iris_epsilon_0.1_lr_0.05_seed_9\n",
      "File exists, pass.\n",
      "dataset_iris_epsilon_0.1_lr_0.01_seed_0\n",
      "File exists, pass.\n",
      "dataset_iris_epsilon_0.1_lr_0.01_seed_1\n",
      "File exists, pass.\n",
      "dataset_iris_epsilon_0.1_lr_0.01_seed_2\n",
      "File exists, pass.\n",
      "dataset_iris_epsilon_0.1_lr_0.01_seed_3\n",
      "File exists, pass.\n",
      "dataset_iris_epsilon_0.1_lr_0.01_seed_4\n",
      "File exists, pass.\n",
      "dataset_iris_epsilon_0.1_lr_0.01_seed_5\n",
      "File exists, pass.\n",
      "dataset_iris_epsilon_0.1_lr_0.01_seed_6\n",
      "File exists, pass.\n",
      "dataset_iris_epsilon_0.1_lr_0.01_seed_7\n",
      "File exists, pass.\n",
      "dataset_iris_epsilon_0.1_lr_0.01_seed_8\n",
      "File exists, pass.\n",
      "dataset_iris_epsilon_0.1_lr_0.01_seed_9\n",
      "File exists, pass.\n",
      "dataset_iris_epsilon_0.1_lr_0.005_seed_0\n",
      "File exists, pass.\n",
      "dataset_iris_epsilon_0.1_lr_0.005_seed_1\n",
      "File exists, pass.\n",
      "dataset_iris_epsilon_0.1_lr_0.005_seed_2\n",
      "File exists, pass.\n",
      "dataset_iris_epsilon_0.1_lr_0.005_seed_3\n",
      "File exists, pass.\n",
      "dataset_iris_epsilon_0.1_lr_0.005_seed_4\n",
      "File exists, pass.\n",
      "dataset_iris_epsilon_0.1_lr_0.005_seed_5\n",
      "File exists, pass.\n",
      "dataset_iris_epsilon_0.1_lr_0.005_seed_6\n",
      "File exists, pass.\n",
      "dataset_iris_epsilon_0.1_lr_0.005_seed_7\n",
      "File exists, pass.\n",
      "dataset_iris_epsilon_0.1_lr_0.005_seed_8\n",
      "File exists, pass.\n",
      "dataset_iris_epsilon_0.1_lr_0.005_seed_9\n",
      "File exists, pass.\n",
      "dataset_iris_epsilon_0.1_lr_0.001_seed_0\n",
      "File exists, pass.\n",
      "dataset_iris_epsilon_0.1_lr_0.001_seed_1\n",
      "File exists, pass.\n",
      "dataset_iris_epsilon_0.1_lr_0.001_seed_2\n",
      "File exists, pass.\n",
      "dataset_iris_epsilon_0.1_lr_0.001_seed_3\n",
      "File exists, pass.\n",
      "dataset_iris_epsilon_0.1_lr_0.001_seed_4\n",
      "File exists, pass.\n",
      "dataset_iris_epsilon_0.1_lr_0.001_seed_5\n",
      "File exists, pass.\n",
      "dataset_iris_epsilon_0.1_lr_0.001_seed_6\n",
      "File exists, pass.\n",
      "dataset_iris_epsilon_0.1_lr_0.001_seed_7\n",
      "File exists, pass.\n",
      "dataset_iris_epsilon_0.1_lr_0.001_seed_8\n",
      "File exists, pass.\n",
      "dataset_iris_epsilon_0.1_lr_0.001_seed_9\n",
      "File exists, pass.\n",
      "dataset_mammographic_epsilon_0.1_lr_0.05_seed_0\n",
      "File exists, pass.\n",
      "dataset_mammographic_epsilon_0.1_lr_0.05_seed_1\n",
      "File exists, pass.\n",
      "dataset_mammographic_epsilon_0.1_lr_0.05_seed_2\n",
      "File exists, pass.\n",
      "dataset_mammographic_epsilon_0.1_lr_0.05_seed_3\n",
      "File exists, pass.\n",
      "dataset_mammographic_epsilon_0.1_lr_0.05_seed_4\n",
      "File exists, pass.\n",
      "dataset_mammographic_epsilon_0.1_lr_0.05_seed_5\n",
      "File exists, pass.\n",
      "dataset_mammographic_epsilon_0.1_lr_0.05_seed_6\n",
      "File exists, pass.\n",
      "dataset_mammographic_epsilon_0.1_lr_0.05_seed_7\n",
      "File exists, pass.\n",
      "dataset_mammographic_epsilon_0.1_lr_0.05_seed_8\n",
      "File exists, pass.\n",
      "dataset_mammographic_epsilon_0.1_lr_0.05_seed_9\n",
      "File exists, pass.\n",
      "dataset_mammographic_epsilon_0.1_lr_0.01_seed_0\n",
      "File exists, pass.\n",
      "dataset_mammographic_epsilon_0.1_lr_0.01_seed_1\n",
      "File exists, pass.\n",
      "dataset_mammographic_epsilon_0.1_lr_0.01_seed_2\n",
      "File exists, pass.\n",
      "dataset_mammographic_epsilon_0.1_lr_0.01_seed_3\n",
      "File exists, pass.\n",
      "dataset_mammographic_epsilon_0.1_lr_0.01_seed_4\n",
      "File exists, pass.\n",
      "dataset_mammographic_epsilon_0.1_lr_0.01_seed_5\n",
      "File exists, pass.\n",
      "dataset_mammographic_epsilon_0.1_lr_0.01_seed_6\n",
      "File exists, pass.\n",
      "dataset_mammographic_epsilon_0.1_lr_0.01_seed_7\n",
      "File exists, pass.\n",
      "dataset_mammographic_epsilon_0.1_lr_0.01_seed_8\n",
      "File exists, pass.\n",
      "dataset_mammographic_epsilon_0.1_lr_0.01_seed_9\n",
      "File exists, pass.\n",
      "dataset_mammographic_epsilon_0.1_lr_0.005_seed_0\n",
      "File exists, pass.\n",
      "dataset_mammographic_epsilon_0.1_lr_0.005_seed_1\n",
      "File exists, pass.\n",
      "dataset_mammographic_epsilon_0.1_lr_0.005_seed_2\n",
      "File exists, pass.\n",
      "dataset_mammographic_epsilon_0.1_lr_0.005_seed_3\n",
      "File exists, pass.\n",
      "dataset_mammographic_epsilon_0.1_lr_0.005_seed_4\n",
      "File exists, pass.\n",
      "dataset_mammographic_epsilon_0.1_lr_0.005_seed_5\n",
      "File exists, pass.\n",
      "dataset_mammographic_epsilon_0.1_lr_0.005_seed_6\n",
      "File exists, pass.\n",
      "dataset_mammographic_epsilon_0.1_lr_0.005_seed_7\n",
      "File exists, pass.\n",
      "dataset_mammographic_epsilon_0.1_lr_0.005_seed_8\n",
      "File exists, pass.\n",
      "dataset_mammographic_epsilon_0.1_lr_0.005_seed_9\n",
      "File exists, pass.\n",
      "dataset_mammographic_epsilon_0.1_lr_0.001_seed_0\n",
      "File exists, pass.\n",
      "dataset_mammographic_epsilon_0.1_lr_0.001_seed_1\n",
      "File exists, pass.\n",
      "dataset_mammographic_epsilon_0.1_lr_0.001_seed_2\n",
      "File exists, pass.\n",
      "dataset_mammographic_epsilon_0.1_lr_0.001_seed_3\n",
      "File exists, pass.\n",
      "dataset_mammographic_epsilon_0.1_lr_0.001_seed_4\n",
      "File exists, pass.\n",
      "dataset_mammographic_epsilon_0.1_lr_0.001_seed_5\n",
      "File exists, pass.\n",
      "dataset_mammographic_epsilon_0.1_lr_0.001_seed_6\n",
      "File exists, pass.\n",
      "dataset_mammographic_epsilon_0.1_lr_0.001_seed_7\n",
      "File exists, pass.\n",
      "dataset_mammographic_epsilon_0.1_lr_0.001_seed_8\n",
      "File exists, pass.\n",
      "dataset_mammographic_epsilon_0.1_lr_0.001_seed_9\n",
      "File exists, pass.\n",
      "dataset_Pendigits_epsilon_0.1_lr_0.05_seed_0\n",
      "File exists, pass.\n",
      "dataset_Pendigits_epsilon_0.1_lr_0.05_seed_1\n",
      "File exists, pass.\n",
      "dataset_Pendigits_epsilon_0.1_lr_0.05_seed_2\n",
      "File exists, pass.\n",
      "dataset_Pendigits_epsilon_0.1_lr_0.05_seed_3\n",
      "File exists, pass.\n",
      "dataset_Pendigits_epsilon_0.1_lr_0.05_seed_4\n",
      "File exists, pass.\n",
      "dataset_Pendigits_epsilon_0.1_lr_0.05_seed_5\n",
      "File exists, pass.\n",
      "dataset_Pendigits_epsilon_0.1_lr_0.05_seed_6\n",
      "File exists, pass.\n",
      "dataset_Pendigits_epsilon_0.1_lr_0.05_seed_7\n",
      "File exists, pass.\n",
      "dataset_Pendigits_epsilon_0.1_lr_0.05_seed_8\n",
      "File exists, pass.\n",
      "dataset_Pendigits_epsilon_0.1_lr_0.05_seed_9\n",
      "File exists, pass.\n",
      "dataset_Pendigits_epsilon_0.1_lr_0.01_seed_0\n",
      "File exists, pass.\n",
      "dataset_Pendigits_epsilon_0.1_lr_0.01_seed_1\n",
      "File exists, pass.\n",
      "dataset_Pendigits_epsilon_0.1_lr_0.01_seed_2\n",
      "File exists, pass.\n",
      "dataset_Pendigits_epsilon_0.1_lr_0.01_seed_3\n",
      "File exists, pass.\n",
      "dataset_Pendigits_epsilon_0.1_lr_0.01_seed_4\n",
      "File exists, pass.\n",
      "dataset_Pendigits_epsilon_0.1_lr_0.01_seed_5\n",
      "File exists, pass.\n",
      "dataset_Pendigits_epsilon_0.1_lr_0.01_seed_6\n",
      "File exists, pass.\n",
      "dataset_Pendigits_epsilon_0.1_lr_0.01_seed_7\n",
      "File exists, pass.\n",
      "dataset_Pendigits_epsilon_0.1_lr_0.01_seed_8\n",
      "File exists, pass.\n",
      "dataset_Pendigits_epsilon_0.1_lr_0.01_seed_9\n",
      "File exists, pass.\n",
      "dataset_Pendigits_epsilon_0.1_lr_0.005_seed_0\n",
      "File exists, pass.\n",
      "dataset_Pendigits_epsilon_0.1_lr_0.005_seed_1\n",
      "File exists, pass.\n",
      "dataset_Pendigits_epsilon_0.1_lr_0.005_seed_2\n",
      "File exists, pass.\n",
      "dataset_Pendigits_epsilon_0.1_lr_0.005_seed_3\n",
      "File exists, pass.\n",
      "dataset_Pendigits_epsilon_0.1_lr_0.005_seed_4\n",
      "File exists, pass.\n",
      "dataset_Pendigits_epsilon_0.1_lr_0.005_seed_5\n",
      "File exists, pass.\n",
      "dataset_Pendigits_epsilon_0.1_lr_0.005_seed_6\n",
      "File exists, pass.\n",
      "dataset_Pendigits_epsilon_0.1_lr_0.005_seed_7\n",
      "File exists, pass.\n",
      "dataset_Pendigits_epsilon_0.1_lr_0.005_seed_8\n",
      "File exists, pass.\n",
      "dataset_Pendigits_epsilon_0.1_lr_0.005_seed_9\n",
      "File exists, pass.\n",
      "dataset_Pendigits_epsilon_0.1_lr_0.001_seed_0\n",
      "File exists, pass.\n",
      "dataset_Pendigits_epsilon_0.1_lr_0.001_seed_1\n",
      "File exists, pass.\n",
      "dataset_Pendigits_epsilon_0.1_lr_0.001_seed_2\n",
      "File exists, pass.\n",
      "dataset_Pendigits_epsilon_0.1_lr_0.001_seed_3\n",
      "File exists, pass.\n",
      "dataset_Pendigits_epsilon_0.1_lr_0.001_seed_4\n",
      "File exists, pass.\n",
      "dataset_Pendigits_epsilon_0.1_lr_0.001_seed_5\n",
      "File exists, pass.\n",
      "dataset_Pendigits_epsilon_0.1_lr_0.001_seed_6\n",
      "File exists, pass.\n",
      "dataset_Pendigits_epsilon_0.1_lr_0.001_seed_7\n",
      "File exists, pass.\n",
      "dataset_Pendigits_epsilon_0.1_lr_0.001_seed_8\n",
      "File exists, pass.\n",
      "dataset_Pendigits_epsilon_0.1_lr_0.001_seed_9\n",
      "File exists, pass.\n",
      "dataset_seeds_epsilon_0.1_lr_0.05_seed_0\n",
      "File exists, pass.\n",
      "dataset_seeds_epsilon_0.1_lr_0.05_seed_1\n",
      "File exists, pass.\n",
      "dataset_seeds_epsilon_0.1_lr_0.05_seed_2\n",
      "File exists, pass.\n",
      "dataset_seeds_epsilon_0.1_lr_0.05_seed_3\n",
      "File exists, pass.\n",
      "dataset_seeds_epsilon_0.1_lr_0.05_seed_4\n",
      "File exists, pass.\n",
      "dataset_seeds_epsilon_0.1_lr_0.05_seed_5\n",
      "File exists, pass.\n",
      "dataset_seeds_epsilon_0.1_lr_0.05_seed_6\n",
      "File exists, pass.\n",
      "dataset_seeds_epsilon_0.1_lr_0.05_seed_7\n",
      "File exists, pass.\n",
      "dataset_seeds_epsilon_0.1_lr_0.05_seed_8\n",
      "File exists, pass.\n",
      "dataset_seeds_epsilon_0.1_lr_0.05_seed_9\n",
      "File exists, pass.\n",
      "dataset_seeds_epsilon_0.1_lr_0.01_seed_0\n",
      "File exists, pass.\n",
      "dataset_seeds_epsilon_0.1_lr_0.01_seed_1\n",
      "File exists, pass.\n",
      "dataset_seeds_epsilon_0.1_lr_0.01_seed_2\n",
      "File exists, pass.\n",
      "dataset_seeds_epsilon_0.1_lr_0.01_seed_3\n",
      "File exists, pass.\n",
      "dataset_seeds_epsilon_0.1_lr_0.01_seed_4\n",
      "File exists, pass.\n",
      "dataset_seeds_epsilon_0.1_lr_0.01_seed_5\n",
      "File exists, pass.\n",
      "dataset_seeds_epsilon_0.1_lr_0.01_seed_6\n",
      "File exists, pass.\n",
      "dataset_seeds_epsilon_0.1_lr_0.01_seed_7\n",
      "File exists, pass.\n",
      "dataset_seeds_epsilon_0.1_lr_0.01_seed_8\n",
      "File exists, pass.\n",
      "dataset_seeds_epsilon_0.1_lr_0.01_seed_9\n",
      "File exists, pass.\n",
      "dataset_seeds_epsilon_0.1_lr_0.005_seed_0\n",
      "File exists, pass.\n",
      "dataset_seeds_epsilon_0.1_lr_0.005_seed_1\n",
      "File exists, pass.\n",
      "dataset_seeds_epsilon_0.1_lr_0.005_seed_2\n",
      "File exists, pass.\n",
      "dataset_seeds_epsilon_0.1_lr_0.005_seed_3\n",
      "File exists, pass.\n",
      "dataset_seeds_epsilon_0.1_lr_0.005_seed_4\n",
      "File exists, pass.\n",
      "dataset_seeds_epsilon_0.1_lr_0.005_seed_5\n",
      "File exists, pass.\n",
      "dataset_seeds_epsilon_0.1_lr_0.005_seed_6\n",
      "File exists, pass.\n",
      "dataset_seeds_epsilon_0.1_lr_0.005_seed_7\n",
      "File exists, pass.\n",
      "dataset_seeds_epsilon_0.1_lr_0.005_seed_8\n",
      "File exists, pass.\n",
      "dataset_seeds_epsilon_0.1_lr_0.005_seed_9\n",
      "File exists, pass.\n",
      "dataset_seeds_epsilon_0.1_lr_0.001_seed_0\n",
      "File exists, pass.\n",
      "dataset_seeds_epsilon_0.1_lr_0.001_seed_1\n",
      "File exists, pass.\n",
      "dataset_seeds_epsilon_0.1_lr_0.001_seed_2\n",
      "File exists, pass.\n",
      "dataset_seeds_epsilon_0.1_lr_0.001_seed_3\n",
      "File exists, pass.\n",
      "dataset_seeds_epsilon_0.1_lr_0.001_seed_4\n",
      "File exists, pass.\n",
      "dataset_seeds_epsilon_0.1_lr_0.001_seed_5\n",
      "File exists, pass.\n",
      "dataset_seeds_epsilon_0.1_lr_0.001_seed_6\n",
      "File exists, pass.\n",
      "dataset_seeds_epsilon_0.1_lr_0.001_seed_7\n",
      "File exists, pass.\n",
      "dataset_seeds_epsilon_0.1_lr_0.001_seed_8\n",
      "File exists, pass.\n",
      "dataset_seeds_epsilon_0.1_lr_0.001_seed_9\n",
      "File exists, pass.\n",
      "dataset_tictactoe_epsilon_0.1_lr_0.05_seed_0\n",
      "File exists, pass.\n",
      "dataset_tictactoe_epsilon_0.1_lr_0.05_seed_1\n",
      "File exists, pass.\n",
      "dataset_tictactoe_epsilon_0.1_lr_0.05_seed_2\n",
      "File exists, pass.\n",
      "dataset_tictactoe_epsilon_0.1_lr_0.05_seed_3\n",
      "File exists, pass.\n",
      "dataset_tictactoe_epsilon_0.1_lr_0.05_seed_4\n",
      "File exists, pass.\n",
      "dataset_tictactoe_epsilon_0.1_lr_0.05_seed_5\n",
      "File exists, pass.\n",
      "dataset_tictactoe_epsilon_0.1_lr_0.05_seed_6\n",
      "File exists, pass.\n",
      "dataset_tictactoe_epsilon_0.1_lr_0.05_seed_7\n",
      "File exists, pass.\n",
      "dataset_tictactoe_epsilon_0.1_lr_0.05_seed_8\n",
      "File exists, pass.\n",
      "dataset_tictactoe_epsilon_0.1_lr_0.05_seed_9\n",
      "File exists, pass.\n",
      "dataset_tictactoe_epsilon_0.1_lr_0.01_seed_0\n",
      "File exists, pass.\n",
      "dataset_tictactoe_epsilon_0.1_lr_0.01_seed_1\n",
      "File exists, pass.\n",
      "dataset_tictactoe_epsilon_0.1_lr_0.01_seed_2\n",
      "File exists, pass.\n",
      "dataset_tictactoe_epsilon_0.1_lr_0.01_seed_3\n",
      "File exists, pass.\n",
      "dataset_tictactoe_epsilon_0.1_lr_0.01_seed_4\n",
      "File exists, pass.\n",
      "dataset_tictactoe_epsilon_0.1_lr_0.01_seed_5\n",
      "File exists, pass.\n",
      "dataset_tictactoe_epsilon_0.1_lr_0.01_seed_6\n",
      "File exists, pass.\n",
      "dataset_tictactoe_epsilon_0.1_lr_0.01_seed_7\n",
      "File exists, pass.\n",
      "dataset_tictactoe_epsilon_0.1_lr_0.01_seed_8\n",
      "File exists, pass.\n",
      "dataset_tictactoe_epsilon_0.1_lr_0.01_seed_9\n",
      "File exists, pass.\n",
      "dataset_tictactoe_epsilon_0.1_lr_0.005_seed_0\n",
      "File exists, pass.\n",
      "dataset_tictactoe_epsilon_0.1_lr_0.005_seed_1\n",
      "File exists, pass.\n",
      "dataset_tictactoe_epsilon_0.1_lr_0.005_seed_2\n",
      "File exists, pass.\n",
      "dataset_tictactoe_epsilon_0.1_lr_0.005_seed_3\n",
      "File exists, pass.\n",
      "dataset_tictactoe_epsilon_0.1_lr_0.005_seed_4\n",
      "File exists, pass.\n",
      "dataset_tictactoe_epsilon_0.1_lr_0.005_seed_5\n",
      "File exists, pass.\n",
      "dataset_tictactoe_epsilon_0.1_lr_0.005_seed_6\n",
      "File exists, pass.\n",
      "dataset_tictactoe_epsilon_0.1_lr_0.005_seed_7\n",
      "File exists, pass.\n",
      "dataset_tictactoe_epsilon_0.1_lr_0.005_seed_8\n",
      "File exists, pass.\n",
      "dataset_tictactoe_epsilon_0.1_lr_0.005_seed_9\n",
      "File exists, pass.\n",
      "dataset_tictactoe_epsilon_0.1_lr_0.001_seed_0\n",
      "File exists, pass.\n",
      "dataset_tictactoe_epsilon_0.1_lr_0.001_seed_1\n",
      "File exists, pass.\n",
      "dataset_tictactoe_epsilon_0.1_lr_0.001_seed_2\n",
      "File exists, pass.\n",
      "dataset_tictactoe_epsilon_0.1_lr_0.001_seed_3\n",
      "File exists, pass.\n",
      "dataset_tictactoe_epsilon_0.1_lr_0.001_seed_4\n",
      "File exists, pass.\n",
      "dataset_tictactoe_epsilon_0.1_lr_0.001_seed_5\n",
      "File exists, pass.\n",
      "dataset_tictactoe_epsilon_0.1_lr_0.001_seed_6\n",
      "File exists, pass.\n",
      "dataset_tictactoe_epsilon_0.1_lr_0.001_seed_7\n",
      "File exists, pass.\n",
      "dataset_tictactoe_epsilon_0.1_lr_0.001_seed_8\n",
      "File exists, pass.\n",
      "dataset_tictactoe_epsilon_0.1_lr_0.001_seed_9\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn2clases_epsilon_0.1_lr_0.05_seed_0\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn2clases_epsilon_0.1_lr_0.05_seed_1\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn2clases_epsilon_0.1_lr_0.05_seed_2\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn2clases_epsilon_0.1_lr_0.05_seed_3\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn2clases_epsilon_0.1_lr_0.05_seed_4\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn2clases_epsilon_0.1_lr_0.05_seed_5\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn2clases_epsilon_0.1_lr_0.05_seed_6\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn2clases_epsilon_0.1_lr_0.05_seed_7\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn2clases_epsilon_0.1_lr_0.05_seed_8\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn2clases_epsilon_0.1_lr_0.05_seed_9\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn2clases_epsilon_0.1_lr_0.01_seed_0\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn2clases_epsilon_0.1_lr_0.01_seed_1\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn2clases_epsilon_0.1_lr_0.01_seed_2\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn2clases_epsilon_0.1_lr_0.01_seed_3\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn2clases_epsilon_0.1_lr_0.01_seed_4\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn2clases_epsilon_0.1_lr_0.01_seed_5\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn2clases_epsilon_0.1_lr_0.01_seed_6\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn2clases_epsilon_0.1_lr_0.01_seed_7\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn2clases_epsilon_0.1_lr_0.01_seed_8\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn2clases_epsilon_0.1_lr_0.01_seed_9\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn2clases_epsilon_0.1_lr_0.005_seed_0\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn2clases_epsilon_0.1_lr_0.005_seed_1\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn2clases_epsilon_0.1_lr_0.005_seed_2\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn2clases_epsilon_0.1_lr_0.005_seed_3\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn2clases_epsilon_0.1_lr_0.005_seed_4\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn2clases_epsilon_0.1_lr_0.005_seed_5\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn2clases_epsilon_0.1_lr_0.005_seed_6\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn2clases_epsilon_0.1_lr_0.005_seed_7\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn2clases_epsilon_0.1_lr_0.005_seed_8\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn2clases_epsilon_0.1_lr_0.005_seed_9\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn2clases_epsilon_0.1_lr_0.001_seed_0\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn2clases_epsilon_0.1_lr_0.001_seed_1\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn2clases_epsilon_0.1_lr_0.001_seed_2\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn2clases_epsilon_0.1_lr_0.001_seed_3\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn2clases_epsilon_0.1_lr_0.001_seed_4\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn2clases_epsilon_0.1_lr_0.001_seed_5\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn2clases_epsilon_0.1_lr_0.001_seed_6\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn2clases_epsilon_0.1_lr_0.001_seed_7\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn2clases_epsilon_0.1_lr_0.001_seed_8\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn2clases_epsilon_0.1_lr_0.001_seed_9\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn3clases_epsilon_0.1_lr_0.05_seed_0\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn3clases_epsilon_0.1_lr_0.05_seed_1\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn3clases_epsilon_0.1_lr_0.05_seed_2\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn3clases_epsilon_0.1_lr_0.05_seed_3\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn3clases_epsilon_0.1_lr_0.05_seed_4\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn3clases_epsilon_0.1_lr_0.05_seed_5\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn3clases_epsilon_0.1_lr_0.05_seed_6\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn3clases_epsilon_0.1_lr_0.05_seed_7\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn3clases_epsilon_0.1_lr_0.05_seed_8\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn3clases_epsilon_0.1_lr_0.05_seed_9\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn3clases_epsilon_0.1_lr_0.01_seed_0\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn3clases_epsilon_0.1_lr_0.01_seed_1\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn3clases_epsilon_0.1_lr_0.01_seed_2\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn3clases_epsilon_0.1_lr_0.01_seed_3\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn3clases_epsilon_0.1_lr_0.01_seed_4\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn3clases_epsilon_0.1_lr_0.01_seed_5\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn3clases_epsilon_0.1_lr_0.01_seed_6\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn3clases_epsilon_0.1_lr_0.01_seed_7\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn3clases_epsilon_0.1_lr_0.01_seed_8\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn3clases_epsilon_0.1_lr_0.01_seed_9\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn3clases_epsilon_0.1_lr_0.005_seed_0\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn3clases_epsilon_0.1_lr_0.005_seed_1\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn3clases_epsilon_0.1_lr_0.005_seed_2\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn3clases_epsilon_0.1_lr_0.005_seed_3\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn3clases_epsilon_0.1_lr_0.005_seed_4\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn3clases_epsilon_0.1_lr_0.005_seed_5\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn3clases_epsilon_0.1_lr_0.005_seed_6\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn3clases_epsilon_0.1_lr_0.005_seed_7\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn3clases_epsilon_0.1_lr_0.005_seed_8\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn3clases_epsilon_0.1_lr_0.005_seed_9\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn3clases_epsilon_0.1_lr_0.001_seed_0\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn3clases_epsilon_0.1_lr_0.001_seed_1\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn3clases_epsilon_0.1_lr_0.001_seed_2\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn3clases_epsilon_0.1_lr_0.001_seed_3\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn3clases_epsilon_0.1_lr_0.001_seed_4\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn3clases_epsilon_0.1_lr_0.001_seed_5\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn3clases_epsilon_0.1_lr_0.001_seed_6\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn3clases_epsilon_0.1_lr_0.001_seed_7\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn3clases_epsilon_0.1_lr_0.001_seed_8\n",
      "File exists, pass.\n",
      "dataset_vertebralcolumn3clases_epsilon_0.1_lr_0.001_seed_9\n",
      "File exists, pass.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result = torch.zeros((13, 4, 10))\n",
    "lrs = [0.05, 0.01, 0.005, 0.001]\n",
    "for i in range(len(datasets)):\n",
    "    dataset = datasets[i]\n",
    "    datapath = os.path.join(f'./dataset/{dataset}')\n",
    "    with open(datapath, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    X_train    = data['X_train']\n",
    "    y_train    = data['y_train']\n",
    "    X_valid    = data['X_valid']\n",
    "    y_valid    = data['y_valid']\n",
    "    X_test     = data['X_test']\n",
    "    y_test     = data['y_test']\n",
    "    data_name  = data['name']\n",
    "\n",
    "    N_class    = data['n_class']\n",
    "    N_feature  = data['n_feature']\n",
    "    N_train    = X_train.shape[0]\n",
    "    N_valid    = X_valid.shape[0]\n",
    "    N_test     = X_test.shape[0]\n",
    "    \n",
    "    # generate tensordataset\n",
    "    trainset = TensorDataset(X_train, y_train)\n",
    "    validset = TensorDataset(X_valid, y_valid)\n",
    "    testset  = TensorDataset(X_test, y_test)\n",
    "\n",
    "    # batch\n",
    "    train_loader = DataLoader(trainset, batch_size=N_train)\n",
    "    valid_loader = DataLoader(validset, batch_size=N_valid)\n",
    "    test_loader  = DataLoader(testset,  batch_size=N_test)\n",
    "    \n",
    "    for j in range(len(lrs)):\n",
    "        lr = lrs[j]\n",
    "        \n",
    "        for n in range(10):\n",
    "            seed = n      \n",
    "            setup = f'dataset_{data_name}_epsilon_0.1_lr_{lr}_seed_{seed}'\n",
    "            print(setup)\n",
    "            \n",
    "            if os.path.exists(f'./results_variation/pNN_{setup}'):\n",
    "                print('File exists, pass.')\n",
    "            else:\n",
    "                torch.manual_seed(seed)\n",
    "                model = pNN.pNN([N_feature, 3, N_class], 10, 0.1)\n",
    "    \n",
    "                optimizer = torch.optim.Adam([{'params':model.GetParam('theta_'),'lr':1e-1},\n",
    "                                  {'params':model.GetParam('eta_'), 'lr':lr},\n",
    "                                  {'params':model.GetParam('inv_'),'lr':lr}])\n",
    "    \n",
    "    \n",
    "                NN, acc_valid = training.training_pNN(model, train_loader, valid_loader, optimizer, pNN.lossfunction)\n",
    "                result[i,j,n] = acc_valid\n",
    "                torch.save(NN, f'./results_variation/pNN_{setup}')\n",
    "\n",
    "a = {'acc_valid': result}\n",
    "with open('acc_valid_variation.txt', 'wb') as file:\n",
    "    pickle.dump(a, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0889ade0",
   "metadata": {},
   "source": [
    "## valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5100794",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = torch.zeros((13, 6, 10))\n",
    "lrs = [0.0001, 0.0005,0.001, 0.005, 0.01,0.05]\n",
    "#lrs = [ 0.0005,0.001, 0.005, 0.01,0.05]\n",
    "for i in range(len(datasets)):\n",
    "    dataset = datasets[i]\n",
    "    datapath = os.path.join(f'./dataset/{dataset}')\n",
    "    with open(datapath, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    X_train    = data['X_train']\n",
    "    y_train    = data['y_train']\n",
    "    X_valid    = data['X_valid']\n",
    "    y_valid    = data['y_valid']\n",
    "    X_test     = data['X_test']\n",
    "    y_test     = data['y_test']\n",
    "    data_name  = data['name']\n",
    "\n",
    "    N_class    = data['n_class']\n",
    "    N_feature  = data['n_feature']\n",
    "    N_train    = X_train.shape[0]\n",
    "    N_valid    = X_valid.shape[0]\n",
    "    N_test     = X_test.shape[0]\n",
    "    \n",
    "    # generate tensordataset\n",
    "    trainset = TensorDataset(X_train, y_train)\n",
    "    validset = TensorDataset(X_valid, y_valid)\n",
    "    testset  = TensorDataset(X_test, y_test)\n",
    "\n",
    "    # batch\n",
    "    train_loader = DataLoader(trainset, batch_size=N_train)\n",
    "    valid_loader = DataLoader(validset, batch_size=N_valid)\n",
    "    test_loader  = DataLoader(testset,  batch_size=N_test)\n",
    "    \n",
    "    for j in range(len(lrs)):\n",
    "        lr = lrs[j]\n",
    "        \n",
    "        for n in range(10):\n",
    "            seed = n\n",
    "            \n",
    "            setup = f'dataset_{data_name}_lr_{lr}_seed_{seed}_learnable1'\n",
    "            model = torch.load(f'./results_variation/pNN_{setup}')\n",
    "            #model.SetParameter('N', 100)\n",
    "            \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for X_valid, y_valid in valid_loader:\n",
    "                    #print(setup)\n",
    "                    prediction_valid = model(X_valid)\n",
    "                    #acc_valid = lossfunction(prediction_valid, y_valid)\n",
    "                    yhat_valid = torch.argmax(prediction_valid.data, 2)\n",
    "                \n",
    "                    yy_valid = y_valid.repeat(prediction_valid.shape[0], 1)\n",
    "                    valid_correct = torch.sum(yhat_valid == yy_valid.data)\n",
    "                    acc_valid = valid_correct / (y_valid.numel() * prediction_valid.shape[0])\n",
    "                    result[i,j,n] = acc_valid\n",
    "                \n",
    "\n",
    "#a = {'acc_valid': result}\n",
    "#with open('acc_valid_variation.txt', 'wb') as file:\n",
    "    #pickle.dump(a, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee9942a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([13, 6]),\n",
       " tensor([[1.0000, 1.0000, 0.9217, 1.0000, 1.0000, 1.0000],\n",
       "         [0.9435, 0.9645, 0.9597, 0.9758, 0.9685, 0.9669],\n",
       "         [0.9698, 0.9647, 0.9604, 0.9698, 0.9554, 0.9662],\n",
       "         [0.8851, 0.8542, 0.8533, 0.7601, 0.7965, 0.8123],\n",
       "         [0.8706, 0.8255, 0.8059, 0.8190, 0.8412, 0.8092],\n",
       "         [0.9242, 0.8660, 0.8425, 0.8033, 0.8660, 0.8438],\n",
       "         [0.9862, 0.9862, 0.9552, 0.9759, 0.9103, 0.9414],\n",
       "         [0.8330, 0.7911, 0.6660, 0.8267, 0.8183, 0.8251],\n",
       "         [0.4741, 0.4778, 0.4240, 0.2089, 0.3662, 0.3695],\n",
       "         [0.9561, 0.9195, 0.9146, 0.9317, 0.8951, 0.8976],\n",
       "         [0.9089, 0.8157, 0.7948, 0.8194, 0.7571, 0.7691],\n",
       "         [0.8672, 0.8230, 0.8164, 0.7262, 0.7180, 0.8541],\n",
       "         [0.8590, 0.8311, 0.8426, 0.7443, 0.8410, 0.8279]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1=result.mean(dim = 2)\n",
    "result1.shape, result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b8f5477",
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = result1.mean(dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8b52add6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8829, 0.8553, 0.8275, 0.8124, 0.8257, 0.8372])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cfdbb13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([13, 6]),\n",
       " tensor([[0.0000, 0.0000, 0.1650, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0344, 0.0068, 0.0000, 0.0120, 0.0111, 0.0080],\n",
       "         [0.0046, 0.0086, 0.0051, 0.0057, 0.0095, 0.0059],\n",
       "         [0.0502, 0.0250, 0.0406, 0.0327, 0.0472, 0.0534],\n",
       "         [0.0192, 0.0260, 0.1257, 0.0369, 0.0392, 0.0238],\n",
       "         [0.0247, 0.1234, 0.1413, 0.1712, 0.0587, 0.1120],\n",
       "         [0.0436, 0.0291, 0.0284, 0.0284, 0.0654, 0.0284],\n",
       "         [0.0185, 0.0793, 0.1298, 0.0080, 0.0131, 0.0090],\n",
       "         [0.0996, 0.0431, 0.0488, 0.1303, 0.0767, 0.0830],\n",
       "         [0.0154, 0.0283, 0.0287, 0.0224, 0.0431, 0.1210],\n",
       "         [0.0580, 0.0199, 0.0098, 0.0574, 0.0410, 0.0363],\n",
       "         [0.0332, 0.0344, 0.0344, 0.0934, 0.0456, 0.0706],\n",
       "         [0.0158, 0.0135, 0.0115, 0.2392, 0.0079, 0.0610]]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean= result.std(dim=2)\n",
    "mean.shape, mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00e0d875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0265, 0.0335, 0.0588, 0.0739, 0.0246, 0.0405])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean2 = mean.std(dim =0)\n",
    "mean2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0134f3",
   "metadata": {},
   "source": [
    "## best valid under lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ba6aa4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([13, 10]),\n",
       " tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000],\n",
       "         [0.9839, 0.9839, 0.9839, 0.9597, 0.9032, 0.8871, 0.9435, 0.9355, 0.9355,\n",
       "          0.9194],\n",
       "         [0.9712, 0.9640, 0.9640, 0.9712, 0.9640, 0.9784, 0.9712, 0.9712, 0.9712,\n",
       "          0.9712],\n",
       "         [0.9033, 0.8939, 0.9009, 0.7453, 0.8915, 0.9009, 0.9080, 0.8821, 0.9198,\n",
       "          0.9057],\n",
       "         [0.8497, 0.8693, 0.8693, 0.8366, 0.8497, 0.8889, 0.8824, 0.8824, 0.8889,\n",
       "          0.8889],\n",
       "         [0.8954, 0.9216, 0.9281, 0.9477, 0.9216, 0.9346, 0.9346, 0.8693, 0.9412,\n",
       "          0.9477],\n",
       "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8621,\n",
       "          1.0000],\n",
       "         [0.8220, 0.8115, 0.8168, 0.8220, 0.8482, 0.8377, 0.8482, 0.8482, 0.8115,\n",
       "          0.8639],\n",
       "         [0.3753, 0.4563, 0.4272, 0.4372, 0.3899, 0.5887, 0.6197, 0.4581, 0.6228,\n",
       "          0.3653],\n",
       "         [0.9756, 0.9268, 0.9512, 0.9512, 0.9512, 0.9756, 0.9512, 0.9512, 0.9756,\n",
       "          0.9512],\n",
       "         [0.8220, 0.8586, 0.8848, 0.9005, 0.9372, 0.9686, 0.9005, 0.9791, 0.9895,\n",
       "          0.8482],\n",
       "         [0.8361, 0.8525, 0.8689, 0.8689, 0.8525, 0.8361, 0.8689, 0.8852, 0.9508,\n",
       "          0.8525],\n",
       "         [0.8525, 0.8525, 0.8525, 0.8689, 0.8525, 0.8852, 0.8852, 0.8525, 0.8525,\n",
       "          0.8361]]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_valid = result[:,0,:]\n",
    "best_valid.shape, best_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e12f832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 5, 8, 5, 3, 0, 9, 8, 0, 8, 8, 5])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = best_valid.argmax(dim = 1)\n",
    "index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf9786d",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d25521d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = torch.zeros((13, 6, 10))\n",
    "#lrs = [0.05, 0.01, 0.005, 0.001]\n",
    "lrs = [0.0001, 0.0005,0.001, 0.005, 0.01,0.05]\n",
    "for i in range(len(datasets)):\n",
    "    dataset = datasets[i]\n",
    "    datapath = os.path.join(f'./dataset/{dataset}')\n",
    "    with open(datapath, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    X_train    = data['X_train']\n",
    "    y_train    = data['y_train']\n",
    "    X_valid    = data['X_valid']\n",
    "    y_valid    = data['y_valid']\n",
    "    X_test     = data['X_test']\n",
    "    y_test     = data['y_test']\n",
    "    data_name  = data['name']\n",
    "\n",
    "    N_class    = data['n_class']\n",
    "    N_feature  = data['n_feature']\n",
    "    N_train    = X_train.shape[0]\n",
    "    N_valid    = X_valid.shape[0]\n",
    "    N_test     = X_test.shape[0]\n",
    "    \n",
    "    # generate tensordataset\n",
    "    trainset = TensorDataset(X_train, y_train)\n",
    "    validset = TensorDataset(X_valid, y_valid)\n",
    "    testset  = TensorDataset(X_test, y_test)\n",
    "\n",
    "    # batch\n",
    "    train_loader = DataLoader(trainset, batch_size=N_train)\n",
    "    valid_loader = DataLoader(validset, batch_size=N_valid)\n",
    "    test_loader  = DataLoader(testset,  batch_size=N_test)\n",
    "    \n",
    "    for j in range(len(lrs)):\n",
    "        lr = lrs[j]\n",
    "        \n",
    "        for n in range(10):\n",
    "            seed = n\n",
    "            if lr == 0.001 :\n",
    "                setup = f'dataset_{data_name}_lr_{lr}_seed_{seed}_learnable0.001'\n",
    "            else:\n",
    "                setup = f'dataset_{data_name}_lr_{lr}_seed_{seed}_learnable'\n",
    "            model = torch.load(f'./results_variation/pNN_{setup}')\n",
    "            #model.SetParameter('N', 100)\n",
    "            \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for X_valid, y_valid in test_loader:\n",
    "                    \n",
    "                    prediction_valid = model(X_valid)\n",
    "                    #acc_valid = lossfunction(prediction_valid, y_valid)\n",
    "                    yhat_valid = torch.argmax(prediction_valid.data, 2)\n",
    "                \n",
    "                    yy_valid = y_valid.repeat(prediction_valid.shape[0], 1)\n",
    "                    valid_correct = torch.sum(yhat_valid == yy_valid.data)\n",
    "                    acc_valid = valid_correct / (y_valid.numel() * prediction_valid.shape[0])\n",
    "                    result[i,j,n] = acc_valid\n",
    "                \n",
    "\n",
    "#a = {'acc_valid': result}\n",
    "#with open('acc_valid_variation.txt', 'wb') as file:\n",
    "    #pickle.dump(a, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6924807c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 1.0000, 0.9200, 1.0000, 0.7600, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000],\n",
       "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000],\n",
       "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000],\n",
       "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000],\n",
       "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000]],\n",
       "\n",
       "        [[0.9127, 0.8413, 0.9524, 0.9206, 0.8254, 0.8175, 0.9127, 0.8254,\n",
       "          0.9048, 0.8333],\n",
       "         [0.9524, 0.9048, 0.9365, 0.8968, 0.9048, 0.9444, 0.8968, 0.8492,\n",
       "          0.9444, 0.8254],\n",
       "         [0.8810, 0.9286, 0.9048, 0.8889, 0.9365, 0.8968, 0.8968, 0.9048,\n",
       "          0.9048, 0.8810],\n",
       "         [0.8968, 0.9127, 0.9286, 0.9286, 0.9206, 0.9048, 0.9127, 0.8889,\n",
       "          0.8413, 0.8810],\n",
       "         [0.8889, 0.9206, 0.8333, 0.8968, 0.9048, 0.8889, 0.8889, 0.8254,\n",
       "          0.8413, 0.8254]],\n",
       "\n",
       "        [[0.9714, 0.9643, 0.9571, 0.9643, 0.9714, 0.9714, 0.9714, 0.9500,\n",
       "          0.9643, 0.9643],\n",
       "         [0.9571, 0.9643, 0.9714, 0.9714, 0.9643, 0.9714, 0.9643, 0.9714,\n",
       "          0.9714, 0.9714],\n",
       "         [0.9571, 0.9714, 0.9714, 0.9714, 0.9643, 0.9714, 0.9714, 0.9714,\n",
       "          0.9714, 0.9714],\n",
       "         [0.9714, 0.9643, 0.9643, 0.9643, 0.9714, 0.9643, 0.9714, 0.9714,\n",
       "          0.9714, 0.9714],\n",
       "         [0.9714, 0.9643, 0.9714, 0.9643, 0.9643, 0.9643, 0.9714, 0.9714,\n",
       "          0.9714, 0.9714]],\n",
       "\n",
       "        [[0.8592, 0.8404, 0.8873, 0.8451, 0.8052, 0.7958, 0.8169, 0.8028,\n",
       "          0.8685, 0.7934],\n",
       "         [0.7629, 0.7629, 0.7840, 0.7629, 0.7629, 0.7887, 0.7700, 0.7629,\n",
       "          0.8732, 0.7653],\n",
       "         [0.8028, 0.8427, 0.7653, 0.7629, 0.7629, 0.7629, 0.7629, 0.7629,\n",
       "          0.7629, 0.7629],\n",
       "         [0.8803, 0.8545, 0.7840, 0.8028, 0.7653, 0.7629, 0.7629, 0.7864,\n",
       "          0.7629, 0.7629],\n",
       "         [0.8803, 0.8545, 0.7840, 0.8028, 0.7653, 0.7629, 0.7629, 0.7864,\n",
       "          0.7629, 0.7629]],\n",
       "\n",
       "        [[0.8377, 0.9221, 0.8377, 0.8896, 0.8442, 0.9416, 0.9221, 0.9351,\n",
       "          0.9416, 0.8377],\n",
       "         [0.8442, 0.8636, 0.8312, 0.8377, 0.3377, 0.8442, 0.9091, 0.9416,\n",
       "          0.9416, 0.8312],\n",
       "         [0.8377, 0.8377, 0.9416, 0.9351, 0.8377, 0.8377, 0.8117, 0.8377,\n",
       "          0.9416, 0.8377],\n",
       "         [0.8377, 0.8377, 0.9481, 0.8377, 0.5000, 0.9416, 0.8377, 0.8442,\n",
       "          0.8506, 0.8377],\n",
       "         [0.8377, 0.8377, 0.9416, 0.8377, 0.8961, 0.9416, 0.8377, 0.8377,\n",
       "          0.8377, 0.7857]],\n",
       "\n",
       "        [[0.8831, 0.9221, 0.8766, 0.7273, 0.9091, 0.9221, 0.9221, 0.9286,\n",
       "          0.8831, 0.9026],\n",
       "         [0.9156, 0.8831, 0.8766, 0.8831, 0.7273, 0.8766, 0.5260, 0.8766,\n",
       "          0.7273, 0.8831],\n",
       "         [0.8052, 0.9286, 0.8766, 0.8766, 0.5260, 0.9156, 0.7857, 0.8766,\n",
       "          0.8506, 0.5260],\n",
       "         [0.7857, 0.8636, 0.8766, 0.8052, 0.5260, 0.8766, 0.8766, 0.8766,\n",
       "          0.7273, 0.8831],\n",
       "         [0.8636, 0.8766, 0.8766, 0.8766, 0.7338, 0.8961, 0.8831, 0.8831,\n",
       "          0.8766, 0.8831]],\n",
       "\n",
       "        [[0.9677, 0.9677, 0.9677, 0.9355, 0.9355, 0.9677, 0.9677, 0.9677,\n",
       "          0.9677, 0.9677],\n",
       "         [0.9677, 0.9677, 0.6452, 0.9677, 0.9677, 0.9677, 0.9355, 0.9677,\n",
       "          0.9677, 0.9677],\n",
       "         [0.9355, 0.9677, 0.9032, 0.9677, 0.9677, 0.9677, 0.9355, 0.9677,\n",
       "          0.9355, 0.9677],\n",
       "         [0.9677, 1.0000, 0.8387, 0.9032, 0.9355, 0.9355, 0.9677, 0.6452,\n",
       "          0.9355, 0.9677],\n",
       "         [0.9677, 0.9677, 0.9355, 0.9355, 0.9355, 0.9677, 0.9355, 0.9677,\n",
       "          0.9355, 0.9677]],\n",
       "\n",
       "        [[0.7979, 0.7927, 0.7979, 0.7927, 0.7927, 0.8135, 0.8031, 0.8187,\n",
       "          0.7824, 0.7979],\n",
       "         [0.7772, 0.8083, 0.8083, 0.7876, 0.7772, 0.7876, 0.7824, 0.7876,\n",
       "          0.7927, 0.7772],\n",
       "         [0.7824, 0.7927, 0.7772, 0.7979, 0.7876, 0.7824, 0.7927, 0.7824,\n",
       "          0.7824, 0.7979],\n",
       "         [0.7876, 0.8031, 0.7617, 0.7927, 0.7979, 0.7876, 0.7876, 0.7876,\n",
       "          0.7979, 0.7461],\n",
       "         [0.7876, 0.7979, 0.8083, 0.7979, 0.7876, 0.7979, 0.8031, 0.7513,\n",
       "          0.8031, 0.8031]],\n",
       "\n",
       "        [[0.6780, 0.6367, 0.5452, 0.6189, 0.4848, 0.4839, 0.5393, 0.4893,\n",
       "          0.3051, 0.6380],\n",
       "         [0.1041, 0.0937, 0.1860, 0.1046, 0.5603, 0.1915, 0.4043, 0.2760,\n",
       "          0.1669, 0.1041],\n",
       "         [0.0960, 0.3215, 0.4829, 0.1046, 0.2733, 0.1046, 0.1924, 0.2451,\n",
       "          0.1119, 0.1173],\n",
       "         [0.0960, 0.1041, 0.2760, 0.1046, 0.1605, 0.2920, 0.1046, 0.3442,\n",
       "          0.0969, 0.3179],\n",
       "         [0.0960, 0.1901, 0.4029, 0.1046, 0.2597, 0.1905, 0.3693, 0.1928,\n",
       "          0.3192, 0.0996]],\n",
       "\n",
       "        [[0.8837, 0.9070, 0.9070, 0.9070, 0.9070, 0.9070, 0.9070, 0.9070,\n",
       "          0.9070, 0.9302],\n",
       "         [0.8837, 0.7209, 0.8605, 0.7674, 0.7907, 0.8372, 0.5349, 0.9070,\n",
       "          0.9070, 0.8837],\n",
       "         [0.8605, 0.8372, 0.8140, 0.8605, 0.9070, 0.8140, 0.8837, 0.8837,\n",
       "          0.8837, 0.8837],\n",
       "         [0.9070, 0.8605, 0.9070, 0.9070, 0.7674, 0.8140, 0.5349, 0.8605,\n",
       "          0.8837, 0.8372],\n",
       "         [0.8837, 0.8140, 0.8605, 0.8605, 0.7907, 0.7674, 0.8372, 0.9070,\n",
       "          0.8837, 0.9070]],\n",
       "\n",
       "        [[0.9271, 0.9740, 0.9427, 0.9531, 0.8021, 0.9792, 0.8958, 0.9635,\n",
       "          0.9792, 0.8958],\n",
       "         [0.3698, 0.6302, 1.0000, 1.0000, 0.9375, 0.9635, 0.8594, 0.8802,\n",
       "          0.3698, 0.3698],\n",
       "         [0.6302, 0.8958, 0.6979, 0.6615, 0.7188, 0.7812, 0.8073, 0.8438,\n",
       "          0.7500, 0.8646],\n",
       "         [0.7604, 0.8490, 0.8177, 0.6302, 0.9167, 0.9062, 0.9740, 0.8958,\n",
       "          0.6302, 0.6302],\n",
       "         [0.9896, 0.8073, 0.6875, 0.8906, 0.7552, 0.7396, 0.9948, 0.9010,\n",
       "          0.6302, 0.7188]],\n",
       "\n",
       "        [[0.8254, 0.8254, 0.8413, 0.7302, 0.6984, 0.8095, 0.8254, 0.8095,\n",
       "          0.8254, 0.8095],\n",
       "         [0.7778, 0.8095, 0.8254, 0.8254, 0.6349, 0.8571, 0.6349, 0.8254,\n",
       "          0.7937, 0.7937],\n",
       "         [0.6349, 0.7619, 0.7302, 0.6349, 0.6349, 0.6349, 0.7778, 0.6349,\n",
       "          0.6349, 0.6349],\n",
       "         [0.6349, 0.7937, 0.8413, 0.6349, 0.7937, 0.6349, 0.6349, 0.6349,\n",
       "          0.6349, 0.8095],\n",
       "         [0.7778, 0.6349, 0.8413, 0.6349, 0.6349, 0.7619, 0.7937, 0.6349,\n",
       "          0.6349, 0.7778]],\n",
       "\n",
       "        [[0.7937, 0.8095, 0.8413, 0.7778, 0.8095, 0.8254, 0.8095, 0.8254,\n",
       "          0.8254, 0.7937],\n",
       "         [0.7937, 0.8413, 0.8095, 0.7778, 0.8095, 0.4762, 0.7937, 0.8254,\n",
       "          0.6190, 0.8095],\n",
       "         [0.7937, 0.7937, 0.6825, 0.8413, 0.1587, 0.4762, 0.8095, 0.7937,\n",
       "          0.7937, 0.7937],\n",
       "         [0.7937, 0.8095, 0.8095, 0.7937, 0.6825, 0.6984, 0.7937, 0.7937,\n",
       "          0.8254, 0.8095],\n",
       "         [0.8095, 0.8095, 0.7937, 0.8095, 0.6349, 0.7778, 0.8095, 0.7778,\n",
       "          0.7937, 0.7778]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e17757c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([13, 6]),\n",
       " tensor([[1.0000, 0.9680, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "         [0.8675, 0.8746, 0.9056, 0.9024, 0.9016, 0.8714],\n",
       "         [0.9714, 0.9650, 0.9679, 0.9693, 0.9686, 0.9686],\n",
       "         [0.8756, 0.8315, 0.7796, 0.7751, 0.7925, 0.7925],\n",
       "         [0.9084, 0.8909, 0.8182, 0.8656, 0.8273, 0.8591],\n",
       "         [0.9032, 0.8877, 0.8175, 0.7968, 0.8097, 0.8649],\n",
       "         [0.9032, 0.9613, 0.9323, 0.9516, 0.9097, 0.9516],\n",
       "         [0.8073, 0.7990, 0.7886, 0.7876, 0.7850, 0.7938],\n",
       "         [0.4669, 0.5419, 0.2191, 0.2050, 0.1897, 0.2225],\n",
       "         [0.9023, 0.9070, 0.8093, 0.8628, 0.8279, 0.8512],\n",
       "         [0.8943, 0.9312, 0.7380, 0.7651, 0.8010, 0.8115],\n",
       "         [0.8190, 0.8000, 0.7778, 0.6714, 0.7048, 0.7127],\n",
       "         [0.7984, 0.8111, 0.7556, 0.6937, 0.7810, 0.7794]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean= result.mean(dim=2)\n",
    "mean.shape, mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e394f5e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0773, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0290, 0.0504, 0.0421, 0.0183, 0.0265, 0.0360],\n",
       "        [0.0000, 0.0071, 0.0051, 0.0048, 0.0037, 0.0037],\n",
       "        [0.0407, 0.0333, 0.0343, 0.0268, 0.0422, 0.0422],\n",
       "        [0.0429, 0.0468, 0.1744, 0.0516, 0.1230, 0.0507],\n",
       "        [0.0215, 0.0594, 0.1222, 0.1492, 0.1126, 0.0468],\n",
       "        [0.1075, 0.0136, 0.1014, 0.0228, 0.1029, 0.0170],\n",
       "        [0.0133, 0.0106, 0.0117, 0.0073, 0.0177, 0.0163],\n",
       "        [0.1183, 0.1096, 0.1541, 0.1272, 0.1045, 0.1120],\n",
       "        [0.0147, 0.0110, 0.1149, 0.0319, 0.1124, 0.0480],\n",
       "        [0.0790, 0.0551, 0.2751, 0.0888, 0.1313, 0.1268],\n",
       "        [0.0171, 0.0468, 0.0785, 0.0599, 0.0911, 0.0845],\n",
       "        [0.0168, 0.0190, 0.1160, 0.2157, 0.0490, 0.0526]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean1 = result.std(dim=2)\n",
    "mean1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6807548f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8552, 0.8592, 0.7930, 0.7882, 0.7922, 0.8061])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean2 = mean.mean(dim= 0)\n",
    "mean2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9cd31f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0385, 0.0415, 0.0946, 0.0619, 0.0705, 0.0490])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std = mean1.mean(dim=0)\n",
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c402165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "& $1.0\\pm 0.0$ & $1.0\\pm 0.0$ & $0.922\\pm 0.165$ & $1.0\\pm 0.0$ & $1.0\\pm 0.0$ & $1.0\\pm 0.0$ \n",
      "& $0.944\\pm 0.034$ & $0.965\\pm 0.007$ & $0.96\\pm 0.0$ & $0.976\\pm 0.012$ & $0.969\\pm 0.011$ & $0.967\\pm 0.008$ \n",
      "& $0.97\\pm 0.005$ & $0.965\\pm 0.009$ & $0.96\\pm 0.005$ & $0.97\\pm 0.006$ & $0.955\\pm 0.009$ & $0.966\\pm 0.006$ \n",
      "& $0.885\\pm 0.05$ & $0.854\\pm 0.025$ & $0.853\\pm 0.041$ & $0.76\\pm 0.033$ & $0.796\\pm 0.047$ & $0.812\\pm 0.053$ \n",
      "& $0.871\\pm 0.019$ & $0.825\\pm 0.026$ & $0.806\\pm 0.126$ & $0.819\\pm 0.037$ & $0.841\\pm 0.039$ & $0.809\\pm 0.024$ \n",
      "& $0.924\\pm 0.025$ & $0.866\\pm 0.123$ & $0.842\\pm 0.141$ & $0.803\\pm 0.171$ & $0.866\\pm 0.059$ & $0.844\\pm 0.112$ \n",
      "& $0.986\\pm 0.044$ & $0.986\\pm 0.029$ & $0.955\\pm 0.028$ & $0.976\\pm 0.028$ & $0.91\\pm 0.065$ & $0.941\\pm 0.028$ \n",
      "& $0.833\\pm 0.019$ & $0.791\\pm 0.079$ & $0.666\\pm 0.13$ & $0.827\\pm 0.008$ & $0.818\\pm 0.013$ & $0.825\\pm 0.009$ \n",
      "& $0.474\\pm 0.1$ & $0.478\\pm 0.043$ & $0.424\\pm 0.049$ & $0.209\\pm 0.13$ & $0.366\\pm 0.077$ & $0.37\\pm 0.083$ \n",
      "& $0.956\\pm 0.015$ & $0.92\\pm 0.028$ & $0.915\\pm 0.029$ & $0.932\\pm 0.022$ & $0.895\\pm 0.043$ & $0.898\\pm 0.121$ \n",
      "& $0.909\\pm 0.058$ & $0.816\\pm 0.02$ & $0.795\\pm 0.01$ & $0.819\\pm 0.057$ & $0.757\\pm 0.041$ & $0.769\\pm 0.036$ \n",
      "& $0.867\\pm 0.033$ & $0.823\\pm 0.034$ & $0.816\\pm 0.034$ & $0.726\\pm 0.093$ & $0.718\\pm 0.046$ & $0.854\\pm 0.071$ \n",
      "& $0.859\\pm 0.016$ & $0.831\\pm 0.013$ & $0.843\\pm 0.011$ & $0.744\\pm 0.239$ & $0.841\\pm 0.008$ & $0.828\\pm 0.061$ \n"
     ]
    }
   ],
   "source": [
    "for i in range(13):\n",
    "    s = \"\"\n",
    "    for j in range(6):\n",
    "        s = s+\"& $\" +str(result1[i,j].numpy().round(3)) +\"\\pm \"+str(mean[i,j].numpy().round(3)) +\"$ \"\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c8e7cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "& $0.883\\pm 0.027$ & $0.855\\pm 0.034$ & $0.827\\pm 0.059$ & $0.812\\pm 0.074$ & $0.826\\pm 0.025$ & $0.837\\pm 0.04$ \n"
     ]
    }
   ],
   "source": [
    "s = \"\"\n",
    "for j in range(6):\n",
    "    s = s+\"& $\" +str(result2[j].numpy().round(3)) +\"\\pm \"+str(mean2[j].numpy().round(3)) +\"$ \"\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e61daba",
   "metadata": {},
   "source": [
    "## learnable, N = 100. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4d47a18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic(prediction, y, *args, **kwargs):\n",
    "    act, idx = torch.max(prediction, dim=1)\n",
    "    corrects = (y.view(-1) == idx)\n",
    "    return corrects.float().sum().item() / y.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "89bc1fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BASIC_variation(nn, x, y, *args, **kwargs):\n",
    "    prediction = nn(x)\n",
    "    N = prediction.shape[0]\n",
    "    accs = []\n",
    "    for n in range(N):\n",
    "        accs.append(basic(prediction[n,:,:], y))\n",
    "    return np.mean(accs), np.std(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "85948af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = torch.tensor([0, 0, 5, 8, 5, 3, 0, 9, 8, 0, 8, 8, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c2aa3270",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test1= torch.zeros((13,1,3,2))\n",
    "#result = torch.zeros((13, 6, 10))\n",
    "lrs = [0.0001]\n",
    "epsilons = [0.05, 0.1, 0.3]\n",
    "#lrs = [0.0001, 0.0005,0.001, 0.005, 0.01,0.05]\n",
    "for i in range(len(datasets)):\n",
    "    dataset = datasets[i]\n",
    "    datapath = os.path.join(f'./dataset/{dataset}')\n",
    "    with open(datapath, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    X_train    = data['X_train']\n",
    "    y_train    = data['y_train']\n",
    "    X_valid    = data['X_valid']\n",
    "    y_valid    = data['y_valid']\n",
    "    X_test     = data['X_test']\n",
    "    y_test     = data['y_test']\n",
    "    data_name  = data['name']\n",
    "\n",
    "    N_class    = data['n_class']\n",
    "    N_feature  = data['n_feature']\n",
    "    N_train    = X_train.shape[0]\n",
    "    N_valid    = X_valid.shape[0]\n",
    "    N_test     = X_test.shape[0]\n",
    "    \n",
    "    # generate tensordataset\n",
    "    trainset = TensorDataset(X_train, y_train)\n",
    "    validset = TensorDataset(X_valid, y_valid)\n",
    "    testset  = TensorDataset(X_test, y_test)\n",
    "\n",
    "    # batch\n",
    "    train_loader = DataLoader(trainset, batch_size=N_train)\n",
    "    valid_loader = DataLoader(validset, batch_size=N_valid)\n",
    "    test_loader  = DataLoader(testset,  batch_size=N_test)\n",
    "    \n",
    "    for j in range(len(lrs)):\n",
    "        lr = lrs[j]\n",
    "          \n",
    "        setup = f'dataset_{data_name}_lr_{lr}_seed_{index[i]}_learnable1'\n",
    "        model = torch.load(f'./results_variation/pNN_{setup}')\n",
    "        model.SetParameter('N', 50)\n",
    "        for k in range(len(epsilons)):\n",
    "            epsilon = epsilons[k]\n",
    "            model.SetParameter('epsilon', epsilon)\n",
    "            \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for X_test, y_test in test_loader:\n",
    "                    #print(setup)\n",
    "                    BASIC_variation(model, X_test, y_test)\n",
    "                    acc, std = BASIC_variation(model, X_test, y_test)\n",
    "                    results_test1[i,j,k,0], results_test1[i,j,k,1] = acc, std\n",
    "                \n",
    "\n",
    "#a = {'acc_valid': result}\n",
    "#with open('acc_valid_variation.txt', 'wb') as file:\n",
    "    #pickle.dump(a, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a8396019",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.0000, 0.9107, 0.9351, 0.7512, 0.8136, 0.8401, 0.8681, 0.6897, 0.3410,\n",
       "         0.8821, 0.6332, 0.7206, 0.6416]),\n",
       " tensor([0.0000, 0.0166, 0.0823, 0.2177, 0.1581, 0.1026, 0.1114, 0.1184, 0.1195,\n",
       "         0.0595, 0.2158, 0.0866, 0.1637]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test1[:,0,0,0], results_test1[:,0,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "445f9472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.7713), tensor(0.1117))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test1[:,0,0,0].mean(), results_test1[:,0,0,1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8a778aee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.9880, 0.8975, 0.8117, 0.6103, 0.6325, 0.6964, 0.7735, 0.6267, 0.2471,\n",
       "         0.7395, 0.5695, 0.6744, 0.5243]),\n",
       " tensor([0.0447, 0.0447, 0.2334, 0.2841, 0.2307, 0.2128, 0.1438, 0.1147, 0.1226,\n",
       "         0.2048, 0.1733, 0.0787, 0.1991]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test1[:,0,1,0], results_test1[:,0,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "43d09c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.6763), tensor(0.1606))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test1[:,0,1,0].mean(), results_test1[:,0,1,1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "71655ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.7776, 0.6898, 0.6139, 0.2662, 0.5310, 0.5059, 0.5303, 0.5557, 0.1395,\n",
       "         0.4879, 0.5267, 0.6170, 0.3867]),\n",
       " tensor([0.2366, 0.2488, 0.2330, 0.2800, 0.2266, 0.2194, 0.1819, 0.0681, 0.0698,\n",
       "         0.2262, 0.1278, 0.0841, 0.1695]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test1[:,0,2,0], results_test1[:,0,2,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "62f80408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.4415), tensor(0.3599))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test1[:,0,0].mean(), results_test1[:,0,0].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781e9357",
   "metadata": {},
   "source": [
    "## epsilon= 0, N = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b9aea7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test1= torch.zeros((13,1,2))\n",
    "#result = torch.zeros((13, 6, 10))\n",
    "lrs = [0.0001]\n",
    "#epsilons = [0.05, 0.1, 0.3]\n",
    "#lrs = [0.0001, 0.0005,0.001, 0.005, 0.01,0.05]\n",
    "for i in range(len(datasets)):\n",
    "    dataset = datasets[i]\n",
    "    datapath = os.path.join(f'./dataset/{dataset}')\n",
    "    with open(datapath, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    X_train    = data['X_train']\n",
    "    y_train    = data['y_train']\n",
    "    X_valid    = data['X_valid']\n",
    "    y_valid    = data['y_valid']\n",
    "    X_test     = data['X_test']\n",
    "    y_test     = data['y_test']\n",
    "    data_name  = data['name']\n",
    "\n",
    "    N_class    = data['n_class']\n",
    "    N_feature  = data['n_feature']\n",
    "    N_train    = X_train.shape[0]\n",
    "    N_valid    = X_valid.shape[0]\n",
    "    N_test     = X_test.shape[0]\n",
    "    \n",
    "    # generate tensordataset\n",
    "    trainset = TensorDataset(X_train, y_train)\n",
    "    validset = TensorDataset(X_valid, y_valid)\n",
    "    testset  = TensorDataset(X_test, y_test)\n",
    "\n",
    "    # batch\n",
    "    train_loader = DataLoader(trainset, batch_size=N_train)\n",
    "    valid_loader = DataLoader(validset, batch_size=N_valid)\n",
    "    test_loader  = DataLoader(testset,  batch_size=N_test)\n",
    "    \n",
    "    for j in range(len(lrs)):\n",
    "        lr = lrs[j]\n",
    "          \n",
    "        setup = f'dataset_{data_name}_lr_{lr}_seed_{index[i]}_learnable1'\n",
    "        model = torch.load(f'./results_variation/pNN_{setup}')\n",
    "        #model.SetParameter('N', 50)\n",
    "        \n",
    "            \n",
    "            \n",
    "        with torch.no_grad():\n",
    "            for X_test, y_test in test_loader:\n",
    "                    #print(setup)\n",
    "                BASIC_variation(model, X_test, y_test)\n",
    "                acc, std = BASIC_variation(model, X_test, y_test)\n",
    "                results_test1[i,j,0], results_test1[i,j,1] = acc, std\n",
    "                \n",
    "\n",
    "#a = {'acc_valid': result}\n",
    "#with open('acc_valid_variation.txt', 'wb') as file:\n",
    "    #pickle.dump(a, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "07dc499f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.0000, 0.9365, 0.9714, 0.8991, 0.9351, 0.9221, 0.9677, 0.8083, 0.6198,\n",
       "         0.9070, 1.0000, 0.8254, 0.7937]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test1[:,0,0],results_test1[:,0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d96548",
   "metadata": {},
   "source": [
    "## nonlearnable1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "62de1f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "index2 =torch.tensor([0, 0, 2, 4, 3, 6, 1, 2, 6, 1, 0, 0, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "51e5bd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test= torch.zeros((13,1,3,2))\n",
    "#result = torch.zeros((13, 6, 10))\n",
    "lrs = [0]\n",
    "epsilons = [0.05, 0.1, 0.3]\n",
    "#lrs = [0.0001, 0.0005,0.001, 0.005, 0.01,0.05]\n",
    "for i in range(len(datasets)):\n",
    "    dataset = datasets[i]\n",
    "    datapath = os.path.join(f'./dataset/{dataset}')\n",
    "    with open(datapath, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    X_train    = data['X_train']\n",
    "    y_train    = data['y_train']\n",
    "    X_valid    = data['X_valid']\n",
    "    y_valid    = data['y_valid']\n",
    "    X_test     = data['X_test']\n",
    "    y_test     = data['y_test']\n",
    "    data_name  = data['name']\n",
    "\n",
    "    N_class    = data['n_class']\n",
    "    N_feature  = data['n_feature']\n",
    "    N_train    = X_train.shape[0]\n",
    "    N_valid    = X_valid.shape[0]\n",
    "    N_test     = X_test.shape[0]\n",
    "    \n",
    "    # generate tensordataset\n",
    "    trainset = TensorDataset(X_train, y_train)\n",
    "    validset = TensorDataset(X_valid, y_valid)\n",
    "    testset  = TensorDataset(X_test, y_test)\n",
    "\n",
    "    # batch\n",
    "    train_loader = DataLoader(trainset, batch_size=N_train)\n",
    "    valid_loader = DataLoader(validset, batch_size=N_valid)\n",
    "    test_loader  = DataLoader(testset,  batch_size=N_test)\n",
    "    \n",
    "    for j in range(len(lrs)):\n",
    "        lr = lrs[j]\n",
    "          \n",
    "        setup = f'dataset_{data_name}_seed_{index2[i]}_nonlearnable1'\n",
    "        model = torch.load(f'./results_variation/pNN_{setup}')\n",
    "        model.SetParameter('N', 50)\n",
    "        for k in range(len(epsilons)):\n",
    "            epsilon = epsilons[k]\n",
    "            model.SetParameter('epsilon', epsilon)\n",
    "            \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for X_test, y_test in test_loader:\n",
    "                   \n",
    "                    #BASIC_variation(model, X_test, y_test)\n",
    "                    acc, std = BASIC_variation(model, X_test, y_test)\n",
    "                    results_test[i,j,k,0], results_test[i,j,k,1] = acc, std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e640d56",
   "metadata": {},
   "source": [
    "## N = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e941fddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test= torch.zeros((13,1,2))\n",
    "#result = torch.zeros((13, 6, 10))\n",
    "lrs = [0]\n",
    "#epsilons = [0.05, 0.1, 0.3]\n",
    "#lrs = [0.0001, 0.0005,0.001, 0.005, 0.01,0.05]\n",
    "for i in range(len(datasets)):\n",
    "    dataset = datasets[i]\n",
    "    datapath = os.path.join(f'./dataset/{dataset}')\n",
    "    with open(datapath, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    X_train    = data['X_train']\n",
    "    y_train    = data['y_train']\n",
    "    X_valid    = data['X_valid']\n",
    "    y_valid    = data['y_valid']\n",
    "    X_test     = data['X_test']\n",
    "    y_test     = data['y_test']\n",
    "    data_name  = data['name']\n",
    "\n",
    "    N_class    = data['n_class']\n",
    "    N_feature  = data['n_feature']\n",
    "    N_train    = X_train.shape[0]\n",
    "    N_valid    = X_valid.shape[0]\n",
    "    N_test     = X_test.shape[0]\n",
    "    \n",
    "    # generate tensordataset\n",
    "    trainset = TensorDataset(X_train, y_train)\n",
    "    validset = TensorDataset(X_valid, y_valid)\n",
    "    testset  = TensorDataset(X_test, y_test)\n",
    "\n",
    "    # batch\n",
    "    train_loader = DataLoader(trainset, batch_size=N_train)\n",
    "    valid_loader = DataLoader(validset, batch_size=N_valid)\n",
    "    test_loader  = DataLoader(testset,  batch_size=N_test)\n",
    "    \n",
    "    for j in range(len(lrs)):\n",
    "        lr = lrs[j]\n",
    "          \n",
    "        setup = f'dataset_{data_name}_seed_{index2[i]}_nonlearnable1'\n",
    "        model = torch.load(f'./results_variation/pNN_{setup}')\n",
    "        #model.SetParameter('N', 50)\n",
    "        \n",
    "        #model.SetParameter('epsilon', epsilon)\n",
    "            \n",
    "            \n",
    "        with torch.no_grad():\n",
    "            for X_test, y_test in test_loader:\n",
    "                    #print(setup)\n",
    "                BASIC_variation(model, X_test, y_test)\n",
    "                acc, std = BASIC_variation(model, X_test, y_test)\n",
    "                results_test[i,j,0], results_test[i,j,1] = acc, std\n",
    "                \n",
    "\n",
    "#a = {'acc_valid': result}\n",
    "#with open('acc_valid_variation.txt', 'wb') as file:\n",
    "    #pickle.dump(a, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f657c56c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.0000, 0.8889, 0.9500, 0.8944, 0.9481, 0.8961, 0.9677, 0.8135, 0.5825,\n",
       "         0.8837, 0.7396, 0.8095, 0.8413]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test[:,j,0], results_test[:,j,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "66217faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.9648, 0.9006, 0.8527, 0.8094, 0.8459, 0.7899, 0.8671, 0.6359, 0.3642,\n",
       "         0.8209, 0.6961, 0.7152, 0.6495]),\n",
       " tensor([0.0788, 0.0243, 0.1805, 0.1217, 0.1037, 0.1138, 0.1162, 0.1437, 0.1159,\n",
       "         0.1401, 0.0360, 0.0812, 0.1556]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test[:,0,0,0], results_test[:,0,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b116e58c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.7625), tensor(0.1086))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test[:,0,0,0].mean(), results_test[:,0,0,1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "60a38b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.8764, 0.8663, 0.7066, 0.7227, 0.6901, 0.7064, 0.7168, 0.5602, 0.2511,\n",
       "         0.6247, 0.6490, 0.6565, 0.5176]),\n",
       " tensor([0.1721, 0.1029, 0.2633, 0.2070, 0.1759, 0.1335, 0.1876, 0.1156, 0.1126,\n",
       "         0.2492, 0.0733, 0.1237, 0.1899]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test[:,0,1,0], results_test[:,0,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e4c724f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.6573), tensor(0.1620))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test[:,0,1,0].mean(), results_test[:,0,1,1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6c59bc92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.6212, 0.6467, 0.5849, 0.5465, 0.4557, 0.5355, 0.4968, 0.5066, 0.1661,\n",
       "         0.3949, 0.5612, 0.5759, 0.4146]),\n",
       " tensor([0.2214, 0.2734, 0.2601, 0.2976, 0.1831, 0.2042, 0.1905, 0.0681, 0.0841,\n",
       "         0.2234, 0.1165, 0.1334, 0.1683]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test[:,0,2,0], results_test[:,0,2,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f97ba363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.5005), tensor(0.1865))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test[:,0,2,0].mean(), results_test[:,0,2,1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a2e78b31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acuteinflammation& $0.974\\pm 0.066$ & $1.0\\pm 0.0$\\\\\n",
      "\\hline\n",
      "balancescale& $0.904\\pm 0.013$ & $0.909\\pm 0.019$\\\\\n",
      "\\hline\n",
      "breastcancerwisc& $0.9\\pm 0.137$ & $0.898\\pm 0.144$\\\\\n",
      "\\hline\n",
      "cardiotocography3clases& $0.822\\pm 0.08$ & $0.711\\pm 0.207$\\\\\n",
      "\\hline\n",
      "energyy1& $0.792\\pm 0.135$ & $0.847\\pm 0.106$\\\\\n",
      "\\hline\n",
      "energyy2& $0.772\\pm 0.114$ & $0.843\\pm 0.105$\\\\\n",
      "\\hline\n",
      "iris& $0.865\\pm 0.106$ & $0.847\\pm 0.116$\\\\\n",
      "\\hline\n",
      "mammographic& $0.631\\pm 0.133$ & $0.689\\pm 0.124$\\\\\n",
      "\\hline\n",
      "Pendigits& $0.368\\pm 0.101$ & $0.36\\pm 0.118$\\\\\n",
      "\\hline\n",
      "seeds& $0.817\\pm 0.139$ & $0.877\\pm 0.08$\\\\\n",
      "\\hline\n",
      "tictactoe& $0.686\\pm 0.048$ & $0.673\\pm 0.191$\\\\\n",
      "\\hline\n",
      "vertebralcolumn2clases& $0.711\\pm 0.071$ & $0.71\\pm 0.082$\\\\\n",
      "\\hline\n",
      "vertebralcolumn3clases& $0.656\\pm 0.138$ & $0.686\\pm 0.147$\\\\\n",
      "\\hline\n",
      "& $0.761\\pm 0.099$& $0.773\\pm 0.111$\n",
      "#############################################\n",
      "acuteinflammation& $0.848\\pm 0.164$ & $0.982\\pm 0.059$\\\\\n",
      "\\hline\n",
      "balancescale& $0.882\\pm 0.065$ & $0.868\\pm 0.109$\\\\\n",
      "\\hline\n",
      "breastcancerwisc& $0.719\\pm 0.277$ & $0.794\\pm 0.232$\\\\\n",
      "\\hline\n",
      "cardiotocography3clases& $0.71\\pm 0.231$ & $0.532\\pm 0.326$\\\\\n",
      "\\hline\n",
      "energyy1& $0.643\\pm 0.182$ & $0.624\\pm 0.238$\\\\\n",
      "\\hline\n",
      "energyy2& $0.705\\pm 0.147$ & $0.656\\pm 0.225$\\\\\n",
      "\\hline\n",
      "iris& $0.669\\pm 0.173$ & $0.717\\pm 0.167$\\\\\n",
      "\\hline\n",
      "mammographic& $0.555\\pm 0.106$ & $0.613\\pm 0.114$\\\\\n",
      "\\hline\n",
      "Pendigits& $0.276\\pm 0.105$ & $0.237\\pm 0.125$\\\\\n",
      "\\hline\n",
      "seeds& $0.668\\pm 0.239$ & $0.764\\pm 0.2$\\\\\n",
      "\\hline\n",
      "tictactoe& $0.649\\pm 0.07$ & $0.59\\pm 0.179$\\\\\n",
      "\\hline\n",
      "vertebralcolumn2clases& $0.643\\pm 0.113$ & $0.674\\pm 0.08$\\\\\n",
      "\\hline\n",
      "vertebralcolumn3clases& $0.503\\pm 0.209$ & $0.561\\pm 0.188$\\\\\n",
      "\\hline\n",
      "& $0.651\\pm 0.16$& $0.663\\pm 0.172$\n",
      "#############################################\n",
      "acuteinflammation& $0.664\\pm 0.25$ & $0.804\\pm 0.206$\\\\\n",
      "\\hline\n",
      "balancescale& $0.611\\pm 0.274$ & $0.656\\pm 0.282$\\\\\n",
      "\\hline\n",
      "breastcancerwisc& $0.645\\pm 0.261$ & $0.576\\pm 0.251$\\\\\n",
      "\\hline\n",
      "cardiotocography3clases& $0.423\\pm 0.289$ & $0.353\\pm 0.311$\\\\\n",
      "\\hline\n",
      "energyy1& $0.41\\pm 0.181$ & $0.485\\pm 0.225$\\\\\n",
      "\\hline\n",
      "energyy2& $0.618\\pm 0.21$ & $0.48\\pm 0.206$\\\\\n",
      "\\hline\n",
      "iris& $0.457\\pm 0.16$ & $0.515\\pm 0.188$\\\\\n",
      "\\hline\n",
      "mammographic& $0.513\\pm 0.059$ & $0.567\\pm 0.081$\\\\\n",
      "\\hline\n",
      "Pendigits& $0.171\\pm 0.076$ & $0.118\\pm 0.045$\\\\\n",
      "\\hline\n",
      "seeds& $0.388\\pm 0.238$ & $0.441\\pm 0.228$\\\\\n",
      "\\hline\n",
      "tictactoe& $0.56\\pm 0.12$ & $0.503\\pm 0.136$\\\\\n",
      "\\hline\n",
      "vertebralcolumn2clases& $0.565\\pm 0.121$ & $0.614\\pm 0.085$\\\\\n",
      "\\hline\n",
      "vertebralcolumn3clases& $0.433\\pm 0.156$ & $0.328\\pm 0.158$\\\\\n",
      "\\hline\n",
      "& $0.497\\pm 0.184$& $0.495\\pm 0.185$\n",
      "#############################################\n"
     ]
    }
   ],
   "source": [
    "for j in range(3):\n",
    "    for i in range(13):\n",
    "        s = \"\"\n",
    "    \n",
    "        s = s+\"& $\" +str(results_test[i,0,j,0].numpy().round(3)) +\"\\pm \"+str(results_test[i,0,j,1].numpy().round(3)) +\"$ \" + \"& $\" +str(results_test1[i,0,j,0].numpy().round(3)) +\"\\pm \"+str(results_test1[i,0,j,1].numpy().round(3))+\"$\"\n",
    "        #s = str(datasets[i][:-2]) + s + \"\\\\\" + \"\\\\\"\n",
    "        s = datasets[i][8:-2] + s + \"\\\\\" + \"\\\\\"\n",
    "        print(s)\n",
    "        print(\"\\hline\")\n",
    "    temp = results_test[:,0,j,0].numpy()\n",
    "    Temp = results_test[:,0,j,1].numpy()\n",
    "    temp1 = results_test1[:,0,j,0].numpy()\n",
    "    Temp1 = results_test1[:,0,j,1].numpy()\n",
    "    print(\"& $\" +str(np.mean(temp).round(3))+\"\\pm \"+str(np.mean(Temp).round(3))+\"$\"+\"& $\" +str(np.mean(temp1).round(3))+\"\\pm \"+str(np.mean(Temp1).round(3))+\"$\")\n",
    "    print(\"#############################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "34cdf04f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acuteinflammation& $1.0$ & $1.0$\\\\\n",
      "\\hline\n",
      "balancescale& $0.889$ & $0.937$\\\\\n",
      "\\hline\n",
      "breastcancerwisc& $0.95$ & $0.971$\\\\\n",
      "\\hline\n",
      "cardiotocography3clases& $0.894$ & $0.899$\\\\\n",
      "\\hline\n",
      "energyy1& $0.948$ & $0.935$\\\\\n",
      "\\hline\n",
      "energyy2& $0.896$ & $0.922$\\\\\n",
      "\\hline\n",
      "iris& $0.968$ & $0.968$\\\\\n",
      "\\hline\n",
      "mammographic& $0.813$ & $0.808$\\\\\n",
      "\\hline\n",
      "Pendigits& $0.583$ & $0.62$\\\\\n",
      "\\hline\n",
      "seeds& $0.884$ & $0.907$\\\\\n",
      "\\hline\n",
      "tictactoe& $0.74$ & $1.0$\\\\\n",
      "\\hline\n",
      "vertebralcolumn2clases& $0.81$ & $0.825$\\\\\n",
      "\\hline\n",
      "vertebralcolumn3clases& $0.841$ & $0.794$\\\\\n",
      "\\hline\n",
      "& $0.863$& $0.891$\n",
      "#############################################\n"
     ]
    }
   ],
   "source": [
    "for j in range(1):\n",
    "    for i in range(13):\n",
    "        s = \"\"\n",
    "    \n",
    "        s = s+\"& $\" +str(results_test[i,j,0].numpy().round(3)) +\"$ \" + \"& $\" +str(results_test1[i,j,0].numpy().round(3)) +\"$\"\n",
    "        #s = str(datasets[i][:-2]) + s + \"\\\\\" + \"\\\\\"\n",
    "        s = datasets[i][8:-2] + s + \"\\\\\" + \"\\\\\"\n",
    "        print(s)\n",
    "        print(\"\\hline\")\n",
    "    temp = results_test[:,j,0].numpy()\n",
    "    #Temp = results_test[:,j,1].numpy()\n",
    "    temp1 = results_test1[:,j,0].numpy()\n",
    "    #Temp1 = results_test1[:,j,1].numpy()\n",
    "    print(\"& $\" +str(np.mean(temp).round(3))+\"$\"+\"& $\" +str(np.mean(temp1).round(3))+\"$\")\n",
    "    print(\"#############################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "dc51d44b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-3// 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f1d2dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
